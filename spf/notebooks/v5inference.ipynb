{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77359cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_to_record_matrix(self, thread_idx, record_idx, data):\n",
    "#     tx_pos = self.position_controller.controller.position[\"xy\"][\n",
    "#         self.yaml_config[\"emitter\"][\"motor_channel\"]\n",
    "#     ]\n",
    "#     rx_pos = self.position_controller.controller.position[\"xy\"][\n",
    "#         self.rx_configs[0].motor_channel\n",
    "#     ]\n",
    "\n",
    "#     data.tx_pos_x_mm = tx_pos[0]\n",
    "#     data.tx_pos_y_mm = tx_pos[1]\n",
    "#     data.rx_pos_x_mm = rx_pos[0]\n",
    "#     data.rx_pos_y_mm = rx_pos[1]\n",
    "#     assert data.rx_lo > 1\n",
    "\n",
    "#     if not self.yaml_config[\"dry-run\"]:\n",
    "#         z = self.zarr[f\"receivers/r{thread_idx}\"]\n",
    "#         z.signal_matrix[record_idx] = data.signal_matrix\n",
    "#         for k in v5rx_f64_keys + v5rx_2xf64_keys:\n",
    "#             z[k][record_idx] = getattr(data, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c504d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "from spf.dataset.v5_data import v5rx_2xf64_keys, v5rx_f64_keys, v5rx_new_dataset\n",
    "\n",
    "ds = v5spfdataset(\n",
    "    \"/mnt/md2/2d_wallarray_v2_data/april_nuand/wallarrayv3_2025_04_04_21_06_04_nRX2_rx_random_circle_spacing0p08.zarr\",\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=\"/mnt/md2/cache/precompute_cache_3p5_chunk1/\",\n",
    "    gpu=False,\n",
    "    snapshots_per_session=1,\n",
    "    n_parallel=8,\n",
    "    paired=True,\n",
    "    segmentation_version=3.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(100):\n",
    "#     ds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ffcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5_raw_keys = v5rx_f64_keys + v5rx_2xf64_keys + [\"signal_matrix\"]\n",
    "\n",
    "\n",
    "def data_single_radio_to_raw(d):\n",
    "    return {k: d[k] for k in v5_raw_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16982148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0][0][\"mean_phase_segmentation\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.segmentation import DEFAULT_SEGMENT_ARGS, segment_session\n",
    "\n",
    "v5inf = v5inferencedataset(\n",
    "    yaml_fn=\"/mnt/md2/2d_wallarray_v2_data/april_nuand/wallarrayv3_2025_04_04_21_06_04_nRX2_rx_random_circle_spacing0p08.yaml\",\n",
    "    nthetas=65,\n",
    "    gpu=False,\n",
    "    n_parallel=8,\n",
    "    paired=True,\n",
    "    model_config_fn=\"\",\n",
    "    vehicle_type=\"wallarray\",\n",
    "    skip_segmentation=True,\n",
    ")\n",
    "a = ds[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e13b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"receiver_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0694c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = v5inf.write_to_idx(0, 0, data_single_radio_to_raw(a))\n",
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65707073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"simple_segmentations\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ba944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_entries(a, b):\n",
    "    for k, v in a.items():\n",
    "        if k in training_only_keys:\n",
    "            continue\n",
    "        assert k in b\n",
    "        vp = b[k]\n",
    "        print(k)\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            assert v.isclose(vp, rtol=1e-3).all(), f\"{v} {vp}\"\n",
    "        elif isinstance(v, List):\n",
    "            s = v[0]\n",
    "            sp = vp[0]\n",
    "            assert len(s) == len(sp)\n",
    "            for idx in range(len(s)):\n",
    "                assert s[idx][\"start_idx\"] == sp[idx][\"start_idx\"]\n",
    "                assert s[idx][\"end_idx\"] == sp[idx][\"end_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebbf84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e08645",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fn = \"/mnt/md0/2d_wallarray_v2_data/oct_batch2/wallarrayv3_2024_10_26_01_14_28_nRX2_rx_circle_spacing0p075.zarr\"\n",
    "\n",
    "ds1 = v5spfdataset(\n",
    "    ds_fn,\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=\"/mnt/md2/cache/precompute_cache_3p5_chunk1/\",\n",
    "    gpu=False,\n",
    "    snapshots_per_session=1,\n",
    "    n_parallel=8,\n",
    "    paired=True,\n",
    "    segmentation_version=3.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf65b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = v5spfdataset(\n",
    "    ds_fn,\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=\"/mnt/md2/cache/precompute_cache_3p6/\",\n",
    "    gpu=False,\n",
    "    snapshots_per_session=1,\n",
    "    n_parallel=8,\n",
    "    paired=True,\n",
    "    segmentation_version=3.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f083e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3 = v5spfdataset(\n",
    "    ds_fn,\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=\"/mnt/md2/cache/precompute_cache_3p6x/\",\n",
    "    gpu=False,\n",
    "    snapshots_per_session=1,\n",
    "    n_parallel=8,\n",
    "    paired=True,\n",
    "    segmentation_version=3.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparez(a, b):\n",
    "    for k, v in a.items():\n",
    "        if k not in b:\n",
    "            print(\"B missing\", k)\n",
    "        else:\n",
    "            vb = b[k]\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                if (~v.isfinite()).all():\n",
    "                    pass  # all infinite\n",
    "                elif (v.isclose(vb, rtol=1e-3) * 1.0).mean() > 0.99:\n",
    "                    # print(k,\"pass\")\n",
    "                    # print((v.isclose(vb,rtol=1e-3)*1.0).mean())\n",
    "                    pass\n",
    "                else:\n",
    "                    # print((v.isclose(vb,rtol=1e-3)*1.0).mean())\n",
    "                    print(k, \"fail\", (v.isclose(vb) * 1.0).mean(), v, vb)\n",
    "\n",
    "\n",
    "for idx in range(30):\n",
    "    for ridx in range(2):\n",
    "        z1 = ds1[idx][ridx]\n",
    "        z2 = ds2[idx][ridx]\n",
    "        z3 = ds3[idx][ridx]\n",
    "        comparez(z1, z2)\n",
    "        comparez(z2, z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48583fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "~torch.tensor(torch.inf).isfinite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79a401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2[0][0][\"windowed_beamformer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f2e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1[0][0][\"windowed_beamformer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146027d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438a049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1bbd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import training_only_keys\n",
    "import torch\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def compare_two_entries(a, b):\n",
    "    for k, v in a.items():\n",
    "        if k in training_only_keys + [\"craft_y_rad_binned\"]:\n",
    "            continue\n",
    "        assert k in b, f\"{k} missing!\"\n",
    "        vp = b[k]\n",
    "        print(k)\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            assert v.isclose(vp, rtol=1e-2).all(), f\"{v} {vp}\"\n",
    "        elif isinstance(v, List):\n",
    "            s = v[0]\n",
    "            sp = vp[0]\n",
    "            assert len(s) == len(sp)\n",
    "            for idx in range(len(s)):\n",
    "                assert s[idx][\"start_idx\"] == sp[idx][\"start_idx\"]\n",
    "                assert s[idx][\"end_idx\"] == sp[idx][\"end_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeade0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import v5inferencedataset, data_single_radio_to_raw\n",
    "from spf.dataset.fake_dataset import create_fake_dataset, fake_yaml_v4\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "import pickle\n",
    "from spf.dataset.spf_dataset import v5inferencedataset, data_single_radio_to_raw\n",
    "from spf.dataset.fake_dataset import create_fake_dataset, fake_yaml_v4\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "\n",
    "n = 50\n",
    "tmpdirname = \"/tmp/test1\"\n",
    "fn = tmpdirname + f\"/perfect_circle_n{n}_noise0p0\"\n",
    "\n",
    "create_fake_dataset(filename=fn, yaml_config_str=fake_yaml_v4, n=50, noise=0.01)\n",
    "\n",
    "ds = v5spfdataset(  # make sure everything gets segmented here\n",
    "    fn,\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=tmpdirname,\n",
    "    paired=True,\n",
    "    # skip_fields=set([\"signal_matrix\"]),\n",
    "    segment_if_not_exist=True,\n",
    "    v4=True,\n",
    ")\n",
    "\n",
    "v5inf = v5inferencedataset(\n",
    "    yaml_fn=fn + \".yaml\",\n",
    "    nthetas=65,\n",
    "    gpu=False,\n",
    "    paired=True,\n",
    "    model_config_fn=\"\",\n",
    "    skip_fields=[],\n",
    "    vehicle_type=\"wallarray\",\n",
    "    skip_segmentation=False,\n",
    "    skip_detrend=False,\n",
    ")\n",
    "n = 3\n",
    "\n",
    "for idx in range(n):\n",
    "    d = ds[idx]\n",
    "    for ridx in range(2):\n",
    "        print(idx, ridx)\n",
    "        v5inf.write_to_idx(idx, ridx, data_single_radio_to_raw(d[ridx]))\n",
    "        compare_two_entries(ds[0][1], v5inf[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da2b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0][0][\"craft_y_rad\"], v5inf[0][0][\"craft_y_rad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b40cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.utils import SEGMENTATION_VERSION, load_config, rx_spacing_to_str, to_bin\n",
    "\n",
    "to_bin(v5inf[0][0][\"craft_y_rad\"], 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abedd57e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca1dae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c30be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import v5inferencedataset, data_single_radio_to_raw\n",
    "from spf.dataset.fake_dataset import create_fake_dataset, fake_yaml_v4\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "# create_fake_dataset(filename=fn, yaml_config_str=fake_yaml_v4, n=50, noise=0.01)\n",
    "\n",
    "n = 50\n",
    "tmpdirname = \"/tmp/test1\"\n",
    "fn = tmpdirname + f\"/perfect_circle_n{n}_noise0p0\"\n",
    "ds = v5spfdataset(  # make sure everything gets segmented here\n",
    "    fn,\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=tmpdirname,\n",
    "    paired=True,\n",
    "    # skip_fields=set([\"signal_matrix\"]),\n",
    "    segment_if_not_exist=True,\n",
    "    v4=True,\n",
    ")\n",
    "\n",
    "n = 20\n",
    "precomputed_ds = [ds[i] for i in range(20)]\n",
    "import pickle\n",
    "\n",
    "pickle.dump(precomputed_ds, open(\"/tmp/test1/ds.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9e25e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ad592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from spf.dataset.spf_dataset import v5inferencedataset, data_single_radio_to_raw\n",
    "from spf.dataset.fake_dataset import create_fake_dataset, fake_yaml_v4\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "n = 50\n",
    "tmpdirname = \"/tmp/test1\"\n",
    "fn = tmpdirname + f\"/perfect_circle_n{n}_noise0p0\"\n",
    "\n",
    "v5inf = v5inferencedataset(\n",
    "    yaml_fn=fn + \".yaml\",\n",
    "    nthetas=65,\n",
    "    gpu=False,\n",
    "    n_parallel=8,\n",
    "    paired=True,\n",
    "    model_config_fn=\"\",\n",
    "    vehicle_type=\"wallarray\",\n",
    "    skip_segmentation=True,\n",
    "    skip_detrend=True,\n",
    ")\n",
    "precomputed_ds = pickle.load(open(\"/tmp/test1/ds.pkl\", \"rb\"))\n",
    "for idx in range(20):\n",
    "    d = precomputed_ds[idx]\n",
    "    for ridx in range(2):\n",
    "        print(idx, ridx)\n",
    "        v5inf.write_to_idx(idx, ridx, data_single_radio_to_raw(d[ridx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a042ce12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4697d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = v5spfdataset(  # make sure everything gets segmented here\n",
    "    fn,\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=tmpdirname,\n",
    "    paired=True,\n",
    "    skip_fields=set([\"signal_matrix\"]),\n",
    "    segment_if_not_exist=True,\n",
    "    v4=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c84bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ec1d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d875b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e552f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import v5inferencedataset, data_single_radio_to_raw\n",
    "from spf.dataset.fake_dataset import create_fake_dataset, fake_yaml_v4\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "import pickle\n",
    "from spf.dataset.spf_dataset import v5inferencedataset, data_single_radio_to_raw\n",
    "from spf.dataset.fake_dataset import create_fake_dataset, fake_yaml_v4\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "# create_fake_dataset(filename=fn, yaml_config_str=fake_yaml_v4, n=50, noise=0.01)\n",
    "\n",
    "n = 50\n",
    "tmpdirname = \"/tmp/test1\"\n",
    "fn = tmpdirname + f\"/perfect_circle_n{n}_noise0p0\"\n",
    "\n",
    "ds = v5spfdataset(  # make sure everything gets segmented here\n",
    "    fn,\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=tmpdirname,\n",
    "    paired=True,\n",
    "    # skip_fields=set([\"signal_matrix\"]),\n",
    "    segment_if_not_exist=True,\n",
    "    v4=True,\n",
    ")\n",
    "\n",
    "v5inf = v5inferencedataset(\n",
    "    yaml_fn=fn + \".yaml\",\n",
    "    nthetas=65,\n",
    "    gpu=False,\n",
    "    n_parallel=8,\n",
    "    paired=True,\n",
    "    model_config_fn=\"\",\n",
    "    skip_fields=[],\n",
    "    vehicle_type=\"wallarray\",\n",
    "    skip_segmentation=False,\n",
    "    skip_detrend=False,\n",
    ")\n",
    "n = 3\n",
    "\n",
    "for idx in range(n):\n",
    "    d = ds[idx]\n",
    "    for ridx in range(2):\n",
    "        print(idx, ridx)\n",
    "        v5inf.write_to_idx(idx, ridx, data_single_radio_to_raw(d[ridx]))\n",
    "        # compare_two_entries(ds[0][1], v5inf[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928a36ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import v5inferencedataset, data_single_radio_to_raw\n",
    "from spf.dataset.fake_dataset import create_fake_dataset, fake_yaml_v4\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "import pickle\n",
    "from spf.dataset.spf_dataset import v5inferencedataset, data_single_radio_to_raw\n",
    "from spf.dataset.fake_dataset import create_fake_dataset, fake_yaml_v4\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "# create_fake_dataset(filename=fn, yaml_config_str=fake_yaml_v4, n=50, noise=0.01)\n",
    "import logging\n",
    "from spf.dataset.spf_dataset import v5_collate_keys_fast, global_config_to_keys_used\n",
    "import torch\n",
    "\n",
    "# try doing neural net inference online\n",
    "# get paired checkpoint results\n",
    "from spf.model_training_and_inference.models.single_point_networks_inference import (\n",
    "    load_model_and_config_from_config_fn_and_checkpoint,\n",
    ")\n",
    "\n",
    "paired_checkpoint_fn = \"/home/mouse9911/gits/spf/checkpoints/march16/paired_wd0p02_gains_vehicle_0p2dropout_noroverbounceREAL_lowdrop_x2/best.pth\"\n",
    "paired_config_fn = \"/home/mouse9911/gits/spf/checkpoints/march16/paired_wd0p02_gains_vehicle_0p2dropout_noroverbounceREAL_lowdrop_x2/config.yml\"\n",
    "empirical_dist_fn = \"/home/mouse9911/gits/spf/checkpoints/apr5/single_wd0p02_gains_vehicle_0p2dropout_noroverbounceREAL_lowdrop_x2_updateddata_lowerdrop_rerun_3p6/empirical_dist.pkl\"\n",
    "inference_cache = \"/tmp/infcache\"\n",
    "n = 50\n",
    "tmpdirname = \"/tmp/test1\"\n",
    "fn = tmpdirname + f\"/perfect_circle_n{n}_noise0p0\"\n",
    "\n",
    "ds = v5spfdataset(  # make sure everything gets segmented here\n",
    "    fn,\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=tmpdirname,\n",
    "    paired=True,\n",
    "    empirical_data_fn=empirical_dist_fn,\n",
    "    # skip_fields=set([\"signal_matrix\"]),\n",
    "    segment_if_not_exist=True,\n",
    "    v4=True,\n",
    ")\n",
    "\n",
    "v5inf = v5inferencedataset(\n",
    "    yaml_fn=fn + \".yaml\",\n",
    "    nthetas=65,\n",
    "    gpu=False,\n",
    "    n_parallel=8,\n",
    "    paired=True,\n",
    "    model_config_fn=\"\",\n",
    "    empirical_data_fn=empirical_dist_fn,\n",
    "    skip_fields=[\"signal_matrix\"],\n",
    "    vehicle_type=\"wallarray\",\n",
    "    skip_segmentation=False,\n",
    "    skip_detrend=False,\n",
    ")\n",
    "n = 3\n",
    "\n",
    "for idx in range(n):\n",
    "    d = ds[idx]\n",
    "    for ridx in range(2):\n",
    "        print(idx, ridx)\n",
    "        # v5inf.write_to_idx(idx, ridx, data_single_radio_to_raw(d[ridx]))\n",
    "        v5inf.write_to_idx(idx, ridx, d[ridx])\n",
    "        # compare_two_entries(ds[0][1], v5inf[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8334c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_nn_dataset_wrapper import v5spfdataset_nn_wrapper\n",
    "\n",
    "\n",
    "nn_ds = v5spfdataset_nn_wrapper(\n",
    "    v5inf,\n",
    "    paired_config_fn,\n",
    "    paired_checkpoint_fn,\n",
    "    inference_cache,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6f370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = v5spfdataset(  # make sure everything gets segmented here\n",
    "    fn,\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=tmpdirname,\n",
    "    paired=True,\n",
    "    empirical_data_fn=empirical_dist_fn,\n",
    "    # skip_fields=set([\"signal_matrix\"]),\n",
    "    segment_if_not_exist=True,\n",
    "    v4=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cedbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.model_training_and_inference.models.single_point_networks_inference import (\n",
    "    get_nn_inference_on_ds_and_cache,\n",
    ")\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.makedirs(inference_cache, exist_ok=True)\n",
    "\n",
    "cached_model_inference = get_nn_inference_on_ds_and_cache(\n",
    "    ds_fn=ds.zarr_fn,\n",
    "    config_fn=paired_config_fn,\n",
    "    checkpoint_fn=paired_checkpoint_fn,\n",
    "    device=\"cpu\",\n",
    "    inference_cache=inference_cache,\n",
    "    batch_size=64,\n",
    "    workers=0,\n",
    "    precompute_cache=ds.precompute_cache,\n",
    "    crash_if_not_cached=False,\n",
    "    segmentation_version=ds.segmentation_version,\n",
    "    v4=True,\n",
    ")[\"paired\"]\n",
    "type(cached_model_inference)\n",
    "\n",
    "from spf.dataset.spf_nn_dataset_wrapper import v5spfdataset_nn_wrapper\n",
    "\n",
    "\n",
    "nn_ds = v5spfdataset_nn_wrapper(\n",
    "    ds,\n",
    "    paired_config_fn,\n",
    "    paired_checkpoint_fn,\n",
    "    inference_cache,\n",
    "    device=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699e1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d729b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_ds[0][0][\"paired\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nn_inference_on_ds_and_cache(\n",
    "    ds_fn=ds.zarr_fn,\n",
    "    config_fn=paired_config_fn,\n",
    "    checkpoint_fn=paired_checkpoint_fn,\n",
    "    device=\"cpu\",\n",
    "    inference_cache=inference_cache,\n",
    "    batch_size=64,\n",
    "    workers=0,\n",
    "    precompute_cache=ds.precompute_cache,\n",
    "    crash_if_not_cached=False,\n",
    "    segmentation_version=ds.segmentation_version,\n",
    "    v4=True,\n",
    ")[\"paired\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47901fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import get_empirical_dist\n",
    "\n",
    "get_empirical_dist(v5inf, 0).T.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b04d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "v5inf[0][0][\"mean_phase_segmentation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.filters.particle_dualradio_filter import plot_single_theta_dual_radio\n",
    "from spf.filters.run_filters_on_data import run_PF_single_theta_dual_radio\n",
    "\n",
    "\n",
    "args = {\n",
    "    \"ds\": ds,\n",
    "    \"N\": 1024 * 4,\n",
    "    \"theta_err\": 0.01,\n",
    "    \"theta_dot_err\": 0.01,\n",
    "}\n",
    "result = run_PF_single_theta_dual_radio(**args)\n",
    "assert result[0][\"metrics\"][\"mse_craft_theta\"] < 0.15\n",
    "plot_single_theta_dual_radio(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c844e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
