{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = \"/Users/miskodzamba/Dropbox/research/gits/spf/\"\n",
    "repo_root = \"/home/mouse9911/gits/spf\"\n",
    "import sys\n",
    "\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)  # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "from spf.dataset.v5_data import v5rx_f64_keys, v5rx_2xf64_keys\n",
    "import numpy as np\n",
    "from spf.rf import speed_of_light\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import os\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "#         return pickle.load(open(results_fn, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "\n",
    "ds = v5spfdataset(\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_04_05_22_13_07_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_10_05_03_21_nRX2_rx_circle_tag_rand10_90_rand30_100\"\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_04_10_05_08_55_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_06_19_04_15_nRX2_bounce\",\n",
    "    # \"test_circle2.zarr\",\n",
    "    # f\"{repo_root}/test_circle.zarr\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_18_16_29_28_nRX2_rx_circle.zarr\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_29_20_46_46_nRX2_rx_circle.zarr\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_20_11_59_44_nRX2_bounce.zarr\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_01_21_14_46_nRX2_rx_circle\",\n",
    "    # \"/Users/miskodzamba/Dropbox/research/gits/spf/wallarrayv3_2024_06_02_01_53_23_nRX2_bounce\",\n",
    "    # \"/Users/miskodzamba/Dropbox/research/gits/spf/wallarrayv3_2024_06_02_02_24_58_nRX2_bounce\",\n",
    "    # \"/Users/miskodzamba/Dropbox/research/gits/spf/wallarrayv3_2024_06_02_02_28_23_nRX2_bounce\",\n",
    "    # \"/Users/miskodzamba/Dropbox/research/gits/spf/wallarrayv3_2024_06_02_02_34_03_nRX2_bounce\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_02_05_18_27_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_02_06_47_47_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/allarrayv3_2024_06_02_08_17_31_nRX2_bounce\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_04_19_14_43_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_04_19_44_10_nRX2_bounce\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_04_21_10_37_nRX2_bounce\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_05_02_41_50_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData2/missions/april5/wallarrayv3_2024_06_07_23_43_19_nRX2_bounce\",\n",
    "    # \"/mnt/md1/2d_wallarray_v2_data/june_fix/wallarrayv3_2024_06_10_14_00_06_nRX2_rx_circle\",\n",
    "    # \"/mnt/md2/spf/2d_wallarray_v2_data/june_fix/wallarrayv3_2024_06_03_01_40_27_nRX2_bounce.zarr\",\n",
    "    # \"/mnt/md0/spf/2d_wallarray_v2_data/june/wallarrayv3_2024_06_03_00_30_00_nRX2_rx_circle.zarr\",\n",
    "    # \"/mnt/md1/2d_wallarray_v2_data/june_fix/wallarrayv3_2024_06_03_01_17_43_nRX2_rx_circle.zarr\",\n",
    "    # \"/mnt/md0/spf/2d_wallarray_v2_data/oct_batch2/wallarrayv3_2024_10_20_22_57_36_nRX2_rx_circle_spacing0p075\",\n",
    "    # \"/mnt/md0/spf/2d_wallarray_v2_data/nov/wallarrayv3_2024_11_21_06_32_06_nRX2_rx_circle_spacing0p04.zarr\",\n",
    "    # \"/mnt/md0/spf/2d_wallarray_v2_data/oct_batch2/wallarrayv3_2024_11_11_01_46_58_nRX2_rx_circle_spacing0p043.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/june_fix/wallarrayv3_2024_07_03_17_00_10_nRX2_rx_circle_spacing0p06.zarr\",\n",
    "    # \"/mnt/md0/spf/2d_wallarray_v2_data/june/wallarrayv3_2024_07_30_02_01_37_nRX2_rx_circle_spacing0p06.zarr\",\n",
    "    # \"/mnt/md1/2d_wallarray_v2_data/june_fix/wallarrayv3_2024_07_30_06_31_42_nRX2_bounce_spacing0p06.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/june_fix/wallarrayv3_2024_06_11_21_17_00_nRX2_rx_circle.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/june_fix/wallarrayv3_2024_06_03_01_17_43_nRX2_rx_circle.zarr\",  # ARTIFACT IS NOT HERE\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/aug/wallarrayv3_2024_09_16_06_53_23_nRX2_rx_circle_spacing0p07.zarr\",  # ARTIFACT IS NOT HERE\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/oct_batch2/wallarrayv3_2024_10_20_22_57_36_nRX2_rx_circle_spacing0p075\", # ARTIFACT IS NOT HERE\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/oct_batch2/wallarrayv3_2024_11_07_15_44_22_nRX2_rx_circle_spacing0p075.zarr\", # ARTIFACT IS NOT HERE OPP\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/oct_batch2/wallarrayv3_2024_11_11_23_30_54_nRX2_rx_circle_spacing0p043.zarr\", # ARTIFACT IS NOT HERE ?\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/oct_batch2/wallarrayv3_2024_11_19_14_25_52_nRX2_rx_circle_spacing0p04.zarr\", # ok\n",
    "    #'/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2024_12_24_11_51_36_nRX2_rx_random_circle_spacing0p07.zarr', # ARTIFACT IS HERE\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2025_01_02_19_16_34_nRX2_rx_random_circle_spacing0p07.zarr\",  # ARTIFACT IS HERE\n",
    "    # \"sample_dataset_for_ekf_n65_noise0.0\",\n",
    "    # \"/mnt/md0/spf/rovers/merged/nov20_mission1_rover3.zarr\",\n",
    "    # \"/mnt/md0/spf/rovers/merged/nov20_mission1_rover1.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/dec18_mission1_rover1.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/dec18_mission1_rover3.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/dec25_mission1_rover1.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/dec26_mission2_rover3.zarr\",\n",
    "    # \"/mnt/ssd/rovers/merged/dec28_mission4_rover3.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/dec31_mission1_rover3.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/jan30_mission1_rover1.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/jan30_mission1_rover3.zarr\",\n",
    "    #'/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2024_12_31_12_13_27_nRX2_rx_random_circle_spacing0p07.zarr', # manual ARTIFACT IS HERE\n",
    "    #'/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2024_12_28_06_43_09_nRX2_rx_random_circle_spacing0p07.zarr', #AGC fast attack  ARTIFACT IS HERE\n",
    "    #'/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2025_01_01_18_01_43_nRX2_rx_random_circle_spacing0p07.zarr',  #ARTIFACT IS HERE\n",
    "    #'/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2025_01_01_19_22_58_nRX2_bounce_spacing0p07.zarr',\n",
    "    #'/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2025_01_02_09_06_22_nRX2_rx_random_circle_spacing0p07.zarr',#ARTIFACT IS HERE\n",
    "    #'/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2025_01_02_19_16_34_nRX2_rx_random_circle_spacing0p07.zarr',\n",
    "    #'/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2025_01_02_22_07_57_nRX2_rx_random_circle_spacing0p07.zarr',#ARTIFACT IS HERE\n",
    "    #'/mnt/md2/2d_wallarray_v2_data/dec/wallarrayv3_2025_01_03_16_08_04_nRX2_bounce_spacing0p07.zarr',\n",
    "    # \"/mnt/md2/rovers/merged/jan17_mission1_rover3.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/jan/wallarrayv3_2025_01_30_10_19_29_nRX2_bounce_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/jan/wallarrayv3_2025_01_30_20_33_41_nRX2_rx_random_circle_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/jan/wallarrayv3_2025_02_02_04_06_42_nRX2_rx_random_circle_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/jan/wallarrayv3_2025_02_02_00_50_22_nRX2_bounce_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/jan/wallarrayv3_2025_02_03_01_38_59_nRX2_rx_random_circle_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/jan/wallarrayv3_2025_02_04_20_47_38_nRX2_rx_random_circle_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/jan/wallarrayv3_2025_02_01_09_06_55_nRX2_bounce_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_06_21_20_58_nRX2_bounce_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/feb5_mission1_rover1.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/feb5_mission1_rover3.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_08_01_26_26_nRX2_rx_random_circle_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_08_19_43_20_nRX2_rx_random_circle_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_08_20_53_47_nRX2_bounce_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_10_16_10_44_nRX2_bounce_spacing0p07.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/feb9_mission1_rover3.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/feb12_mission1_rover1.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/feb16_mission1_rover1.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_16_16_40_45_nRX2_rx_random_circle_spacing0p065.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_20_10_25_23_nRX2_rx_random_circle_spacing0p065.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_20_02_29_25_nRX2_rx_random_circle_spacing0p065.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_20_11_42_58_nRX2_bounce_spacing0p065.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_21_01_37_05_nRX2_bounce_spacing0p065.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_21_08_13_40_nRX2_rx_random_circle_spacing0p065.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_21_23_11_13_nRX2_rx_random_circle_spacing0p025.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_22_07_07_41_nRX2_rx_random_circle_spacing0p025.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/feb22_mission7_rover1.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/feb22_mission2_rover1.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_23_04_07_24_nRX2_rx_random_circle_spacing0p025.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/feb/wallarrayv3_2025_02_25_03_25_49_nRX2_rx_random_circle_spacing0p025.zarr\",\n",
    "    # \"/mnt/md2/bladerf_dev/wallarrayv3_2025_03_15_02_16_43_nRX2_bounce_spacing0p025.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged/rover_2025_03_15_21_15_02_nRX2_diamond_spacing0p035_tag_RO1.rover_2025_03_15_21_15_07_nRX1_circle_spacing0p05075_tag_RO2.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged_test/rover_2025_03_15_23_21_47_nRX2_diamond_spacing0p035_tag_RO1.rover_2025_03_15_23_13_53_nRX1_circle_spacing0p05075_tag_RO2.zarr\",\n",
    "    # \"/mnt/md2/rovers/merged_test/rover_2025_03_15_20_03_06_nRX2_diamond_spacing0p035_tag_RO1.rover_2025_03_15_20_02_58_nRX1_circle_spacing0p05075_tag_RO2.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/march/wallarrayv3_2025_03_14_20_54_47_nRX2_bounce_spacing0p043.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/march_nuand/wallarrayv3_2025_03_16_22_07_12_nRX2_rx_random_circle_spacing0p025.zarr\",\n",
    "    \"/mnt/md2/2d_wallarray_v2_data/march_nuand/wallarrayv3_2025_03_17_00_01_12_nRX2_bounce_spacing0p025.zarr\",\n",
    "    # \"/mnt/md2/2d_wallarray_v2_data/march/wallarrayv3_2025_03_15_11_18_30_nRX2_rx_random_circle_spacing0p043.zarr\",\n",
    "    nthetas=65,\n",
    "    ignore_qc=True,\n",
    "    precompute_cache=\"/mnt/md2/cache/precompute_cache_3p5_chunk1/\",\n",
    "    # precompute_cache=\"/mnt/md2/2d_wallarray_v2_data/march_nuand/\",\n",
    "    # precompute_cache=\"/mnt/md2/bladerf_dev/cache/\",\n",
    "    gpu=True,\n",
    "    snapshots_per_session=1,\n",
    "    n_parallel=8,\n",
    "    paired=True,\n",
    "    segmentation_version=3.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.z.receivers.r0.signal_matrix[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.z.tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from spf.rf import torch_pi_norm, pi_norm\n",
    "\n",
    "\n",
    "# show the beamformer vs expected\n",
    "\n",
    "\n",
    "def normalize(x, dim):\n",
    "    # return x\n",
    "    # return x / x.sum(axis=dim, keepdims=True)\n",
    "    return x / x.max(axis=dim, keepdims=True)\n",
    "\n",
    "\n",
    "tx_pos = np.vstack([ds.z.receivers.r0.tx_pos_x_mm, ds.z.receivers.r0.tx_pos_y_mm])\n",
    "rx_pos = np.vstack([ds.z.receivers.r0.rx_pos_x_mm, ds.z.receivers.r0.rx_pos_y_mm])\n",
    "dists = np.sqrt((rx_pos - tx_pos) ** 2).sum(axis=0)\n",
    "\n",
    "\n",
    "if True:\n",
    "    # start_idx = 300\n",
    "    # end_idx = 1000\n",
    "    start_idx = 10\n",
    "    end_idx = min(6000, len(ds))\n",
    "\n",
    "    for rx_idx in range(2):\n",
    "        fig, axs = plt.subplots(5, 1, figsize=(14, 10))\n",
    "\n",
    "        fig.suptitle(f\"{os.path.basename(ds.zarr_fn)}:{start_idx}-{end_idx}\")\n",
    "        ax_idx = 0\n",
    "        axs[ax_idx].set_title(f\"rx_idx{rx_idx} : theta\")\n",
    "        axs[ax_idx].plot(\n",
    "            ds.ground_truth_thetas[rx_idx][start_idx:end_idx], label=\"ground truth\"\n",
    "        )\n",
    "        axs[ax_idx].plot(\n",
    "            torch_pi_norm(\n",
    "                ds.cached_keys[rx_idx][\"rx_heading_in_pis\"][start_idx:end_idx]\n",
    "                * torch.pi\n",
    "            ),\n",
    "            label=\"rx_heading\",\n",
    "        )\n",
    "        axs[ax_idx].plot(\n",
    "            ds.cached_keys[rx_idx][\"rx_theta_in_pis\"][start_idx:end_idx] * torch.pi,\n",
    "            label=\"rx_theta\",\n",
    "        )\n",
    "        axs[ax_idx].set_yticks([-torch.pi, 0, torch.pi], [\"-pi\", \"0\", \"pi\"])\n",
    "        axs[ax_idx].legend()\n",
    "        axs[ax_idx].set_xlim([0, end_idx - start_idx])\n",
    "        ax_idx += 1\n",
    "\n",
    "        axs[ax_idx].set_title(f\"rx_idx{rx_idx} : windowed beamformer\")\n",
    "        x = normalize(\n",
    "            ds.precomputed_zarr[f\"r{rx_idx}\"].windowed_beamformer[:].astype(np.float32),\n",
    "            2,\n",
    "        ).mean(axis=1)[start_idx:end_idx]\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "        axs[ax_idx].imshow(x.T, origin=\"lower\", aspect=\"auto\")\n",
    "        axs[ax_idx].set_yticks([0, 32, 64], [\"-pi\", \"0\", \"pi\"])\n",
    "        ax_idx += 1\n",
    "\n",
    "        axs[ax_idx].set_title(f\"rx_idx{rx_idx} : dist rx to tx\")\n",
    "        axs[ax_idx].plot(dists[start_idx:end_idx])\n",
    "        axs[ax_idx].set_xlim([0, end_idx - start_idx])\n",
    "        ax_idx += 1\n",
    "\n",
    "        axs[ax_idx].set_title(f\"rx_idx{rx_idx} : gain\")\n",
    "        axs[ax_idx].plot(\n",
    "            ds.z.receivers[f\"r{rx_idx}\"].gains[start_idx:end_idx, 0],\n",
    "            label=f\"R{rx_idx}-Gain0\",\n",
    "        )\n",
    "        axs[ax_idx].plot(\n",
    "            ds.z.receivers[f\"r{rx_idx}\"].gains[start_idx:end_idx, 1],\n",
    "            label=f\"R{rx_idx}-Gain1\",\n",
    "        )\n",
    "        axs[ax_idx].set_xlim([0, end_idx - start_idx])\n",
    "        axs[ax_idx].legend()\n",
    "        ax_idx += 1\n",
    "\n",
    "        axs[ax_idx].set_title(f\"rx_idx{rx_idx} : RSSI\")\n",
    "        axs[ax_idx].plot(\n",
    "            ds.z.receivers[f\"r{rx_idx}\"].rssis[start_idx:end_idx, 0],\n",
    "            label=f\"R{rx_idx}-RSSI0\",\n",
    "        )\n",
    "        axs[ax_idx].plot(\n",
    "            ds.z.receivers[f\"r{rx_idx}\"].rssis[start_idx:end_idx, 1],\n",
    "            label=f\"R{rx_idx}-RSSI1\",\n",
    "        )\n",
    "        axs[ax_idx].set_xlim([0, end_idx - start_idx])\n",
    "        axs[ax_idx].legend()\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "step = int((end_idx - start_idx) / 40)\n",
    "sessions = [ds[idx][0] for idx in range(start_idx, end_idx, step)]\n",
    "axs[0].scatter(\n",
    "    [session[\"tx_pos_x_mm\"] for session in sessions],\n",
    "    [session[\"tx_pos_y_mm\"] for session in sessions],\n",
    ")\n",
    "axs[0].scatter(\n",
    "    [session[\"rx_pos_x_mm\"] for session in sessions],\n",
    "    [session[\"rx_pos_y_mm\"] for session in sessions],\n",
    "    label=\"rx\",\n",
    ")\n",
    "axs[1].plot([session[\"tx_pos_x_mm\"] for session in sessions], label=\"tx_x\")\n",
    "axs[1].plot([session[\"tx_pos_y_mm\"] for session in sessions], label=\"tx_y\")\n",
    "axs[1].plot([session[\"rx_pos_x_mm\"] for session in sessions], label=\"rx_x\")\n",
    "axs[1].plot([session[\"rx_pos_y_mm\"] for session in sessions], label=\"rx_y\")\n",
    "axs[1].plot(rx_pos[0][start_idx:end_idx:step], label=\"rx_xx\")\n",
    "axs[1].legend()\n",
    "# fig.gca().set_aspect(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_idx = 150 * 2 + 100\n",
    "session_idx = 1887\n",
    "session_idx = len(ds) - 100\n",
    "session_idx = 5\n",
    "session_idx = 5\n",
    "session_idx = 288\n",
    "# session_idx = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.detrend import detrend_np\n",
    "from spf.rf import beamformer_given_steering_nomean, get_phase_diff, simple_segment\n",
    "import matplotlib.pyplot as plt\n",
    "from spf.dataset.spf_dataset import default_segment_args\n",
    "\n",
    "rx_idx = 0\n",
    "data = ds[session_idx][rx_idx]\n",
    "# signal_matrix = load_zarr_to_numpy(z.receivers[\"r0\"].signal_matrix[session_idx])\n",
    "n = data[\"signal_matrix\"].shape[3]  # 128 * 2048\n",
    "offset = 0\n",
    "# offset = 75000\n",
    "# n = 1024 * 5\n",
    "\n",
    "# offset=135000\n",
    "# n=500\n",
    "# n = data[\"signal_matrix\"][0].shape[2]  # / 4\n",
    "# offset = 50000\n",
    "v = data[\"signal_matrix\"][0, 0, :, offset : offset + n].numpy()\n",
    "v = detrend_np(v)\n",
    "# v = torch.vstack(\n",
    "#    [\n",
    "#        detrend_iq_windows(data[\"signal_matrix\"][0, 0, 0, offset : offset + n],window_size=1024*16),\n",
    "#        detrend_iq_windows(data[\"signal_matrix\"][0, 0, 1, offset : offset + n],window_size=1024*16),\n",
    "#    ]\n",
    "# ).numpy()\n",
    "# v0, sR0, iR0, sI0, iI0 = merge_dynamic_windows_np(\n",
    "#     v[0],\n",
    "#     remove_real=True,\n",
    "#     remove_imag=True,\n",
    "#     window_size=1024,\n",
    "#     merge_similar=True,\n",
    "#     slope_threshold_I=1e-3,\n",
    "#     intercept_threshold_I=1e-2,\n",
    "#     slope_threshold_Q=1e-3,\n",
    "#     intercept_threshold_Q=1e-2,\n",
    "# )\n",
    "# v1, sR1, iR1, sI1, iI1 = merge_dynamic_windows_np(\n",
    "#     v[1],\n",
    "#     remove_real=True,\n",
    "#     remove_imag=True,\n",
    "#     window_size=1024,\n",
    "#     merge_similar=True,\n",
    "#     slope_threshold_I=1e-3,\n",
    "#     intercept_threshold_I=1e-2,\n",
    "#     slope_threshold_Q=1e-3,\n",
    "#     intercept_threshold_Q=1e-2,\n",
    "# )\n",
    "# v = np.vstack([v0, v1])\n",
    "# v = data[\"signal_matrix\"][0].numpy()[0]\n",
    "# print(n,v.shape)\n",
    "pd = get_phase_diff(v)\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 6))\n",
    "fig.suptitle(\n",
    "    f\"Raw signal + Phase offsets: {ds.zarr_fn} rx_idx{rx_idx} sessionidx:{session_idx}\"\n",
    ")\n",
    "axs[0].scatter(np.arange(n), np.abs(v[0]), alpha=0.1, s=1, label=\"ant0\")\n",
    "axs[0].scatter(np.arange(n), v[0].real, alpha=0.1, s=1, label=\"ant0 r\")\n",
    "axs[0].scatter(np.arange(n), v[0].imag, alpha=0.1, s=1, label=\"ant0 i\")\n",
    "axs[0].set_title(\"Raw signal ant0\")\n",
    "axs[1].scatter(np.arange(n), np.abs(v[1]), alpha=0.1, s=1, label=\"ant0\")\n",
    "axs[1].scatter(np.arange(n), v[1].real, alpha=0.1, s=1, label=\"ant0 r\")\n",
    "axs[1].scatter(np.arange(n), v[1].imag, alpha=0.1, s=1, label=\"ant0 i\")\n",
    "axs[1].set_title(\"Raw signal ant1\")\n",
    "axs[0].set_xlabel(\"Sample# (time)\")\n",
    "axs[1].set_xlabel(\"Sample# (time)\")\n",
    "axs[2].set_xlabel(\"Sample# (time)\")\n",
    "axs[2].set_title(\"Phase estimates\")\n",
    "axs[2].scatter(np.arange(n), pd, s=1, alpha=0.01)\n",
    "\n",
    "beam_sds = [\n",
    "    beamformer_given_steering_nomean(\n",
    "        steering_vectors=ds.steering_vectors[receiver_idx],\n",
    "        signal_matrix=v,\n",
    "    )\n",
    "    for receiver_idx in range(2)\n",
    "]\n",
    "\n",
    "window_sds = []\n",
    "# for window in simple_segment(\n",
    "#     v,\n",
    "#     window_size=2500,\n",
    "#     stride=2500,\n",
    "#     trim=20,\n",
    "#     mean_diff_threshold=0.2,  #\n",
    "#     max_stddev_threshold=0.5,  # just eyeballed this\n",
    "#     drop_less_than_size=3000,\n",
    "#     min_abs_signal=40,\n",
    "# )[\"simple_segmentation\"]:\n",
    "for window in simple_segment(v, **default_segment_args)[\"simple_segmentation\"]:\n",
    "    print(window)\n",
    "    if window[\"type\"] == \"signal\":\n",
    "        axs[1].plot(\n",
    "            [window[\"start_idx\"], window[\"end_idx\"]],\n",
    "            [window[\"mean\"], window[\"mean\"]],\n",
    "            color=\"red\",\n",
    "        )\n",
    "    else:\n",
    "        axs[1].plot(\n",
    "            [window[\"start_idx\"], window[\"end_idx\"]],\n",
    "            [window[\"mean\"], window[\"mean\"]],\n",
    "            color=\"orange\",\n",
    "        )\n",
    "    # print(window[\"start_idx\"], window[\"end_idx\"])\n",
    "    _beam_sds = beam_sds[0][:, window[\"start_idx\"] : window[\"end_idx\"]].mean(axis=1)\n",
    "    # _beam_sds = _beam_sds.mean(axis=1)\n",
    "    # _beam_sds -= _beam_sds.min()\n",
    "    window_sds.append(_beam_sds)\n",
    "window_sds = np.array(window_sds)\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed = torch.nn.functional.normalize(\n",
    "    ds[session_idx][rx_idx][\"windowed_beamformer\"][0, 0, :], p=1, dim=1\n",
    ")\n",
    "unnormed = ds[session_idx][rx_idx][\"windowed_beamformer\"][0, 0, :]\n",
    "buffer_size = ds[session_idx][rx_idx][\"signal_matrix\"].shape[-1]\n",
    "window_size = buffer_size // normed.shape[0]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "axs[0].imshow(normed.T, aspect=\"auto\", origin=\"lower\")\n",
    "axs[1].imshow(unnormed.T, aspect=\"auto\", origin=\"lower\")\n",
    "y_ticks = [0, 32, 64]\n",
    "y_labels = [r\"$-\\pi$\", \"0\", r\"$\\pi$\"]\n",
    "x_labels = np.arange(0, buffer_size, 100000)\n",
    "x_indices = x_labels // 2048\n",
    "for idx in [0, 1]:\n",
    "    axs[idx].set_yticks(y_ticks)\n",
    "    axs[idx].set_yticklabels(y_labels)\n",
    "    axs[idx].set_xticks(x_indices)  # Show every 8th tick\n",
    "    axs[idx].set_xticklabels(x_labels)\n",
    "    axs[idx].set_xlabel(\"Sample # (time)\")\n",
    "    axs[idx].set_ylabel(\"power at angle (rad)\")\n",
    "    if idx == 0:\n",
    "        axs[idx].set_title(\"Normalized Beamformer mean by window (size=2048)\")\n",
    "    else:\n",
    "        axs[idx].set_title(\"Beamformer mean by window (size=2048)\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 70000\n",
    "n = 1024 * 8\n",
    "offset = 208896\n",
    "offset = 470000\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[1].plot(\n",
    "    data[\"signal_matrix\"][0, 0, 1, offset : (offset + 1024 * 16)].imag,\n",
    "    label=\"ant1 imag\",\n",
    ")\n",
    "axs[0].plot(\n",
    "    data[\"signal_matrix\"][0, 0, 1, offset : (offset + 1024 * 16)].real,\n",
    "    label=\"ant1 real\",\n",
    ")\n",
    "axs[1].plot(\n",
    "    data[\"signal_matrix\"][0, 0, 0, offset : (offset + 1024 * 16)].imag,\n",
    "    label=\"ant0 imag\",\n",
    ")\n",
    "axs[0].plot(\n",
    "    data[\"signal_matrix\"][0, 0, 0, offset : (offset + 1024 * 16)].real,\n",
    "    label=\"ant0 real\",\n",
    ")\n",
    "axs[0].set_ylabel(\"I value\")\n",
    "axs[1].set_ylabel(\"Q value\")\n",
    "for idx in [0, 1]:\n",
    "    axs[idx].set_xlabel(\"idx in buffer\")\n",
    "    axs[idx].legend()\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"{os.path.basename(ds.zarr_fn)}:{session_idx}:{offset}+{n} rx_idx{rx_idx} IQ values\"\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf import detrend\n",
    "from spf.rf import torch_pi_norm\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "\n",
    "def high_pass_filter(data, cutoff=10, fs=16000000, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype=\"high\", analog=False)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "\n",
    "offset = 75000\n",
    "# offset = 50000\n",
    "n = 5000\n",
    "# session_idx = 287  # 395#+790\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "for rx_idx in range(2):\n",
    "    _data = ds[session_idx][rx_idx]\n",
    "    axs[rx_idx].set_title(\n",
    "        f\"{os.path.basename(ds.zarr_fn)}:{session_idx}:{offset}+{n} PlutoPlus:{rx_idx}, signal phase\"\n",
    "    )\n",
    "    rx0_mean = _data[\"signal_matrix\"][0, 0, 0, offset : (offset + n)].mean()\n",
    "    rx1_mean = _data[\"signal_matrix\"][0, 0, 1, offset : (offset + n)].mean()\n",
    "    axs[rx_idx].plot(\n",
    "        (_data[\"signal_matrix\"][0, 0] - rx0_mean).angle()[0, offset : (offset + n)],\n",
    "        label=\"rx0\",\n",
    "    )\n",
    "    axs[rx_idx].plot(\n",
    "        (_data[\"signal_matrix\"][0, 0] - rx1_mean).angle()[1, offset : (offset + n)],\n",
    "        label=\"rx1\",\n",
    "    )\n",
    "    axs[rx_idx].set_xlabel(\"IDX in captured buffer\")\n",
    "    axs[rx_idx].set_ylabel(\"Measured phase\")\n",
    "    axs[rx_idx].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = ds[session_idx][0]\n",
    "sessions = [ds[idx][0] for idx in range(session_idx - 600, session_idx + 600, 20)]\n",
    "plt.scatter(\n",
    "    [session[\"tx_pos_x_mm\"] for session in sessions],\n",
    "    [session[\"tx_pos_y_mm\"] for session in sessions],\n",
    ")\n",
    "plt.scatter(\n",
    "    [session[\"rx_pos_x_mm\"] for session in sessions],\n",
    "    [session[\"rx_pos_y_mm\"] for session in sessions],\n",
    "    label=\"rx\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "session[\"tx_pos_x_mm\"], session[\"tx_pos_y_mm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"signal_matrix\"][0, 0][1, offset : (offset + n)].mean()\n",
    "offset = 75000\n",
    "# offset = 50000\n",
    "n = 15000\n",
    "offset = 208896\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "for rx_idx in range(2):\n",
    "    data = ds[session_idx][rx_idx]\n",
    "    axs[rx_idx].set_title(\n",
    "        f\"{os.path.basename(ds.zarr_fn)}:{session_idx}:{offset}+{n} PlutoPlus:{rx_idx}\"\n",
    "    )\n",
    "    v = detrend_np(data[\"signal_matrix\"][0, 0])\n",
    "    # v = data[\"signal_matrix\"][0, 0]\n",
    "    axs[rx_idx].plot(v[0, offset : (offset + n)].real, label=\"ant0-real\")\n",
    "    axs[rx_idx].plot(v[0, offset : (offset + n)].imag, label=\"ant0-imag\")\n",
    "    axs[rx_idx].plot(v[1, offset : (offset + n)].real, label=\"ant1-real\")\n",
    "    axs[rx_idx].plot(v[1, offset : (offset + n)].imag, label=\"ant1-imag\")\n",
    "    axs[rx_idx].set_xlabel(\"IDX in captured buffer\")\n",
    "    axs[rx_idx].set_ylabel(\"Measured real/imag (IQ)\")\n",
    "    axs[rx_idx].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import torch_pi_norm\n",
    "\n",
    "\n",
    "dists = (\n",
    "    (ds.cached_keys[0][\"tx_pos_mm\"] / 1000 - ds.cached_keys[0][\"rx_pos_mm\"] / 1000)\n",
    "    .pow(2)\n",
    "    .sum(axis=1)\n",
    "    .sqrt()\n",
    ")\n",
    "# plt.plot(dists)\n",
    "s = 900\n",
    "e = 1100\n",
    "s = 0\n",
    "e = -1\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "for rx_idx in range(2):\n",
    "    axs[rx_idx].plot(\n",
    "        ds.z.receivers[f\"r{rx_idx}\"].gains[s:e, 0], label=f\"R{rx_idx}-Gain0\"\n",
    "    )\n",
    "    axs[rx_idx].plot(\n",
    "        ds.z.receivers[f\"r{rx_idx}\"].gains[s:e, 1], label=f\"R{rx_idx}-Gain1\"\n",
    "    )\n",
    "    axs[rx_idx].legend()\n",
    "    axs[rx_idx].set_xlabel(\"Buffer capture # / IDX\")\n",
    "    axs[rx_idx].set_ylabel(\"Gain\")\n",
    "    axs[rx_idx].set_title(\n",
    "        f\"{os.path.basename(ds.zarr_fn)}:{session_idx}:{offset}+{n} PlutoPlus:{rx_idx}, Gain\"\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import torch_pi_norm\n",
    "\n",
    "\n",
    "dists = (\n",
    "    (ds.cached_keys[0][\"tx_pos_mm\"] / 1000 - ds.cached_keys[0][\"rx_pos_mm\"] / 1000)\n",
    "    .pow(2)\n",
    "    .sum(axis=1)\n",
    "    .sqrt()\n",
    ")\n",
    "# plt.plot(dists)\n",
    "s = 900\n",
    "e = 1100\n",
    "s = 0\n",
    "e = -1\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
    "for rx_idx in range(2):\n",
    "    axs[rx_idx].plot(\n",
    "        ds.z.receivers[f\"r{rx_idx}\"].rssis[s:e, 0], label=f\"R{rx_idx}-RSSI0\"\n",
    "    )\n",
    "    axs[rx_idx].plot(\n",
    "        ds.z.receivers[f\"r{rx_idx}\"].rssis[s:e, 1], label=f\"R{rx_idx}-RSSI1\"\n",
    "    )\n",
    "    axs[rx_idx].legend()\n",
    "    axs[rx_idx].set_xlabel(\"Buffer capture # / IDX\")\n",
    "    axs[rx_idx].set_ylabel(\"RSSI\")\n",
    "    axs[rx_idx].set_title(\n",
    "        f\"{os.path.basename(ds.zarr_fn)}:{session_idx}:{offset}+{n} PlutoPlus:{rx_idx}, RSSI\"\n",
    "    )\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = simple_segment(v, **default_segment_args)[\"simple_segmentation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import torch_pi_norm\n",
    "\n",
    "\n",
    "dists = (\n",
    "    (ds.cached_keys[0][\"tx_pos_mm\"] / 1000 - ds.cached_keys[0][\"rx_pos_mm\"] / 1000)\n",
    "    .pow(2)\n",
    "    .sum(axis=1)\n",
    "    .sqrt()\n",
    ")\n",
    "plt.plot(dists)  # [670:700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ds.z.receivers.r0.gains[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = (\n",
    "    (ds.cached_keys[0][\"tx_pos_mm\"] / 1000 - ds.cached_keys[0][\"rx_pos_mm\"] / 1000)\n",
    "    .pow(2)\n",
    "    .sum(axis=1)\n",
    "    .sqrt()\n",
    ")\n",
    "plt.plot(dists)\n",
    "plt.plot(ds.z.receivers.r0.rssis[:, 0], label=\"R0-rssi0\")\n",
    "plt.plot(ds.z.receivers.r0.rssis[:, 1], label=\"R0-rssi1\")\n",
    "plt.plot(ds.z.receivers.r1.rssis[:, 0], label=\"R1-rssi0\")\n",
    "plt.plot(ds.z.receivers.r1.rssis[:, 1], label=\"R1-rssi1\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import torch_pi_norm\n",
    "\n",
    "\n",
    "dists = (\n",
    "    (ds.cached_keys[0][\"tx_pos_mm\"] / 1000 - ds.cached_keys[0][\"rx_pos_mm\"] / 1000)\n",
    "    .pow(2)\n",
    "    .sum(axis=1)\n",
    "    .sqrt()\n",
    ")\n",
    "\n",
    "r1_err = torch_pi_norm(ds.mean_phase[\"r1\"] - ds.ground_truth_phis[1]).abs()\n",
    "r0_err = torch_pi_norm(ds.mean_phase[\"r0\"] - ds.ground_truth_phis[0]).abs()\n",
    "plt.title(\"Distance vs phi error\")\n",
    "plt.scatter(dists, r1_err, s=1, label=\"R1\")\n",
    "plt.scatter(dists, r0_err, s=1)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Distance (m)\")\n",
    "plt.ylabel(\"abs phi error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-torch.sin(ds.ground_truth_thetas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ground_truth_thetas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import torch_pi_norm\n",
    "\n",
    "\n",
    "r1_err = torch_pi_norm(ds.mean_phase[\"r1\"] - ds.ground_truth_phis[1])\n",
    "r0_err = torch_pi_norm(ds.mean_phase[\"r0\"] - ds.ground_truth_phis[0])\n",
    "plt.hist(r1_err)\n",
    "plt.hist(r0_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import torch_pi_norm\n",
    "\n",
    "\n",
    "r1_err = torch_pi_norm(ds.mean_phase[\"r1\"] - ds.ground_truth_phis[1]).abs()\n",
    "r0_err = torch_pi_norm(ds.mean_phase[\"r0\"] - ds.ground_truth_phis[0]).abs()\n",
    "plt.title(\"Tx pos x vs phi error\")\n",
    "plt.scatter(ds.cached_keys[0][\"tx_pos_mm\"][:, 0], r1_err, s=1)\n",
    "plt.scatter(ds.cached_keys[0][\"tx_pos_mm\"][:, 0], r0_err, s=1)\n",
    "plt.xlabel(\"Distance (m)\")\n",
    "plt.ylabel(\"abs phi error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import mean_phase_mean\n",
    "\n",
    "mean_phases = []\n",
    "means = []\n",
    "weights = []\n",
    "for x in segmentation:\n",
    "    if x[\"type\"] == \"signal\":\n",
    "        means.append(x[\"mean\"])\n",
    "        weights.append(\n",
    "            (x[\"end_idx\"] - x[\"start_idx\"])\n",
    "            * x[\"abs_signal_median\"]\n",
    "            / (x[\"stddev\"] + 1e-6)  # weight by signal strength and region\n",
    "        )\n",
    "if len(means) == 0:\n",
    "    mean_phases.append(torch.nan)\n",
    "else:\n",
    "    means = np.array(means)\n",
    "    weights = np.array(weights)\n",
    "    # weights /= weights.sum()\n",
    "    mean_phases.append(mean_phase_mean(angles=means, weights=weights))\n",
    "mean_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed = torch.nn.functional.normalize(\n",
    "    ds[session_idx][0][\"windowed_beamformer\"][0, 0, :], p=1, dim=1\n",
    ")\n",
    "plt.imshow(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ds[session_idx][0][\"windowed_beamformer\"][0, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[session_idx][0][\"windowed_beamformer\"][0, 0, :].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.receiver_data[0][\"rx_pos_x_mm\"][0], ds.receiver_data[0][\"rx_pos_y_mm\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import pi_norm\n",
    "\n",
    "\n",
    "ridx = 0\n",
    "rx_theta_in_pis = ds.receiver_data[ridx][\"rx_theta_in_pis\"]\n",
    "tx_pos = np.array(\n",
    "    [\n",
    "        ds.receiver_data[ridx][\"tx_pos_x_mm\"],\n",
    "        ds.receiver_data[ridx][\"tx_pos_y_mm\"],\n",
    "    ]\n",
    ")\n",
    "rx_pos = np.array(\n",
    "    [\n",
    "        ds.receiver_data[ridx][\"rx_pos_x_mm\"],\n",
    "        ds.receiver_data[ridx][\"rx_pos_y_mm\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute the angle of the tx with respect to rx\n",
    "d = tx_pos - rx_pos\n",
    "\n",
    "rx_to_tx_theta = np.arctan2(d[0], d[1])\n",
    "# theta = pi_norm(rx_to_tx_theta - rx_theta_in_pis[:] * np.pi)\n",
    "# theta, ds.get_ground_truth_thetas()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((ds.mean_phase[\"r0\"] == 0.0000) * 1.0).mean(), (\n",
    "    (ds.mean_phase[\"r0\"].isfinite()) * 1.0\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.cached_keys[0][\"rx_heading_in_pis\"].shape, ds.ground_truth_phis[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import torch_pi_norm\n",
    "\n",
    "\n",
    "def get_ground_truth_phisX(ds):\n",
    "    ground_truth_phis = []\n",
    "    for ridx in range(ds.n_receivers):\n",
    "        ground_truth_phis.append(\n",
    "            torch_pi_norm(\n",
    "                -torch.sin(\n",
    "                    ds.ground_truth_thetas[ridx]  # this is theta relative to our array!\n",
    "                    # + self.receiver_data[ridx][\"rx_theta_in_pis\"][:] * np.pi\n",
    "                )  # up to negative sign, which way do we spin?\n",
    "                # or maybe this is the order of the receivers 0/1 vs 1/0 on the x-axis\n",
    "                # pretty sure this (-) is more about which receiver is closer to x+/ish\n",
    "                # a -1 here is the same as -rx_spacing!\n",
    "                * ds.rx_wavelength_spacing\n",
    "                * 2\n",
    "                * torch.pi\n",
    "            )\n",
    "        )\n",
    "    return torch.vstack(ground_truth_phis)\n",
    "\n",
    "\n",
    "# z=get_ground_truth_phis(ds)\n",
    "# z==ds.ground_truth_phis\n",
    "# ds.cached_keys[0][\"rx_heading_in_pis\"][:first_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.cached_keys[0][\"rx_heading_in_pis\"][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ground_truth_thetas[0][20], ds.ground_truth_phis[0][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import torch_pi_norm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# segmentation = ds.get_segmentation()\n",
    "\n",
    "\n",
    "first_n = 500 * 4  # 12 * 8\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axs[0].scatter(range(first_n), ds.mean_phase[\"r0\"][:first_n], s=3, label=\"Rx0\")\n",
    "axs[1].scatter(range(first_n), ds.mean_phase[\"r1\"][:first_n], s=3, label=\"Rx1\")\n",
    "axs[0].scatter(\n",
    "    range(first_n),\n",
    "    ds.ground_truth_phis[0][:first_n],\n",
    "    s=3,\n",
    "    label=\"Rx0 (GT)\",\n",
    ")\n",
    "axs[1].scatter(range(first_n), ds.ground_truth_phis[1][:first_n], s=3, label=\"Rx1 (GT)\")\n",
    "for idx in range(2):\n",
    "    axs[idx].legend()\n",
    "    # axs.axvline(x=115)\n",
    "    axs[idx].set_title(\"Mean segmented phase diff\")\n",
    "    axs[idx].set_xlabel(\"Chunk (time)\")\n",
    "    axs[idx].set_ylabel(\"Mean phase diff of seg. chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_by_receiver.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.get_segmentation_mean_phase()\n",
    "ds.get_estimated_thetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.mean_phase[\"r0\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import pi_norm\n",
    "from spf.rf import c as speed_of_light\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "estimated_thetas = ds.get_estimated_thetas()\n",
    "for rx_idx in [0, 1]:\n",
    "\n",
    "    axs[rx_idx].scatter(\n",
    "        range(estimated_thetas[f\"r{rx_idx}\"][0].shape[0]),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][0]),\n",
    "        s=0.4,\n",
    "    )\n",
    "    axs[rx_idx].scatter(\n",
    "        range(estimated_thetas[f\"r{rx_idx}\"][1].shape[0]),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][1]),\n",
    "        s=0.4,\n",
    "    )\n",
    "    axs[rx_idx].scatter(\n",
    "        range(estimated_thetas[f\"r{rx_idx}\"][2].shape[0]),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][2]),\n",
    "        s=0.4,\n",
    "    )\n",
    "    axs[rx_idx].set_xlabel(\"Chunk\")\n",
    "    axs[rx_idx].set_ylabel(\"estimated theta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import pi_norm\n",
    "from spf.rf import reduce_theta_to_positive_y\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "first_n = 1500\n",
    "estimated_thetas = ds.get_estimated_thetas()\n",
    "for rx_idx in [0, 1]:\n",
    "    expected_theta = ds.ground_truth_thetas[rx_idx]\n",
    "    axs[rx_idx].plot(\n",
    "        expected_theta[:first_n],\n",
    "        alpha=0.5,\n",
    "        color=\"red\",\n",
    "        label=\"ground truth\",\n",
    "    )\n",
    "    axs[rx_idx].plot(\n",
    "        reduce_theta_to_positive_y(expected_theta[:first_n]),\n",
    "        alpha=0.5,\n",
    "        color=\"green\",\n",
    "        label=\"reduced ground truth\",\n",
    "    )\n",
    "\n",
    "    n = estimated_thetas[f\"r{rx_idx}\"][0].shape[0]\n",
    "    axs[rx_idx].scatter(\n",
    "        range(first_n),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][0])[:first_n],\n",
    "        s=3,\n",
    "        label=f\"Rx{rx_idx}_peak1\",\n",
    "    )\n",
    "    axs[rx_idx].scatter(\n",
    "        range(first_n),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][1])[:first_n],\n",
    "        s=3,\n",
    "        label=f\"Rx{rx_idx}_peak2\",\n",
    "    )\n",
    "    axs[rx_idx].set_xlabel(\"Chunk\")\n",
    "    axs[rx_idx].set_ylabel(\"estimated theta\")\n",
    "    axs[rx_idx].legend()\n",
    "    axs[rx_idx].set_title(f\"Receiver (Rx) {rx_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = \"/Users/miskodzamba/Dropbox/research/gits/spf/\"\n",
    "import sys\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)  # go to parent dir\n",
    "\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "\n",
    "ds = v5spfdataset(\n",
    "    \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_06_19_04_15_nRX2_bounce\",\n",
    "    nthetas=11,\n",
    ")\n",
    "\n",
    "from functools import cache\n",
    "import gc\n",
    "\n",
    "from spf.dataset.spf_dataset import v5_collate_beamsegnet, v5_thetas_to_targets\n",
    "from spf.model_training_and_inference.models.beamsegnet import (\n",
    "    BeamNSegNetDirect,\n",
    "    BeamNSegNetDiscrete,\n",
    "    # BeamNetDirect,\n",
    "    UNet1D,\n",
    "    ConvNet,\n",
    ")\n",
    "\n",
    "torch_device = torch.device(\"cpu\")\n",
    "nthetas = 11\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": 4,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0,\n",
    "    \"collate_fn\": v5_collate_beamsegnet,\n",
    "}\n",
    "torch.manual_seed(1337)\n",
    "train_dataloader = torch.utils.data.DataLoader(ds, **dataloader_params)\n",
    "\n",
    "import random\n",
    "\n",
    "w = False\n",
    "if w:\n",
    "\n",
    "    import wandb\n",
    "\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"projectspf\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"beamsegnet1\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@cache\n",
    "def mean_guess(shape):\n",
    "    return torch.nn.functional.normalize(torch.ones(shape), p=1, dim=1)\n",
    "\n",
    "\n",
    "X, Y_rad, segmentation = next(iter(train_dataloader))\n",
    "\n",
    "\n",
    "def batch_to_gt_segmentation(X, Y_rad, segmentation):\n",
    "    n, _, samples_per_session = X.shape\n",
    "    window_size = 2048\n",
    "    stride = 2048\n",
    "    assert window_size == stride\n",
    "    assert samples_per_session % window_size == 0\n",
    "    n_windows = samples_per_session // window_size\n",
    "    window_status = torch.zeros(n, n_windows)\n",
    "    for row_idx in range(len(segmentation)):\n",
    "        for window in segmentation[row_idx][\"simple_segmentation\"]:\n",
    "            window_status[\n",
    "                row_idx,\n",
    "                window[\"start_idx\"] // window_size : window[\"end_idx\"] // window_size,\n",
    "            ] = 1\n",
    "    return window_status[:, None]\n",
    "\n",
    "\n",
    "def segmentation_mask(X, segmentations):\n",
    "    seg_mask = torch.zeros(\n",
    "        X.shape[0], X.shape[2], device=X.device\n",
    "    )  # X.new(X.shape[0], X.shape[2])\n",
    "    for row_idx in range(seg_mask.shape[0]):\n",
    "        for w in segmentations[row_idx][\"simple_segmentation\"]:\n",
    "            seg_mask[row_idx, w[\"start_idx\"] : w[\"end_idx\"]] = 1\n",
    "    return seg_mask[:, None]  # orch.nn.functional.normalize(seg_mask, p=1, dim=1)\n",
    "\n",
    "\n",
    "# m = BeamNSegNetDiscrete(nthetas=nthetas, symmetry=False).to(torch_device)\n",
    "# m = BeamNSegNetDirect(nthetas=nthetas, symmetry=False).to(torch_device)\n",
    "# print(\"ALL\", segmentation[0][\"all_windows_stats\"].shape)\n",
    "m = UNet1D().to(torch_device).double()\n",
    "# m = ConvNet(in_channels=3, out_channels=1, hidden=32)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.00001, weight_decay=0)\n",
    "step = 0\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "X = X.double().to(torch_device)\n",
    "# X[:, :2] /= 500\n",
    "for epoch in range(10000):\n",
    "    # for X, Y_rad, segmentation in train_dataloader:\n",
    "    if True:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # full\n",
    "        input = X.clone().to(torch_device)\n",
    "        output = m(input)\n",
    "\n",
    "        seg_mask = segmentation_mask(X, segmentation)\n",
    "        print(input.shape, output.shape, seg_mask.shape)\n",
    "\n",
    "        # downsampled\n",
    "        # input = torch.Tensor(\n",
    "        #     np.vstack(\n",
    "        #         [\n",
    "        #             segmentation[idx][\"all_windows_stats\"].transpose()[None]\n",
    "        #             for idx in range(len(segmentation))\n",
    "        #         ]\n",
    "        #     )\n",
    "        # )\n",
    "        # input[:, 2] /= 50\n",
    "        # output = m(input)\n",
    "        # seg_mask = batch_to_gt_segmentation(X, Y_rad, segmentation)\n",
    "\n",
    "        loss = ((output - seg_mask) ** 2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        to_log = {\"loss\": loss.item()}\n",
    "\n",
    "        _input = input.cpu()\n",
    "        _output = output.cpu().detach().numpy()\n",
    "        first_n = 3000\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(loss.item())\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "            s = 0.3\n",
    "            axs[0].set_title(\"input (track 0/1)\")\n",
    "            axs[0].scatter(range(first_n), _input[0, 0, :first_n], s=s)\n",
    "            axs[0].scatter(range(first_n), _input[0, 1, :first_n], s=s)\n",
    "            axs[1].set_title(\"input (track 2)\")\n",
    "            axs[1].scatter(range(first_n), _input[0, 2, :first_n], s=s)\n",
    "            # mw = mask_weights.cpu().detach().numpy()\n",
    "\n",
    "            axs[2].set_title(\"output vs gt\")\n",
    "            axs[2].scatter(range(first_n), _output[0, 0, :first_n], s=s)\n",
    "            axs[2].scatter(\n",
    "                range(first_n), seg_mask.cpu().detach().numpy()[0, 0, :first_n], s=s\n",
    "            )\n",
    "            to_log[\"fig\"] = fig\n",
    "        if w:\n",
    "            wandb.log(to_log)\n",
    "        step += 1\n",
    "\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape, seg_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = \"/Users/miskodzamba/Dropbox/research/gits/spf/\"\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)  # go to parent dir\n",
    "\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch_device = torch.device(\"cpu\")\n",
    "nthetas = 11\n",
    "lr = 0.001\n",
    "batch_size = 8\n",
    "\n",
    "ds = v5spfdataset(\n",
    "    \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_06_19_04_15_nRX2_bounce\",\n",
    "    nthetas=11,\n",
    ")\n",
    "\n",
    "from functools import cache\n",
    "import gc\n",
    "\n",
    "from spf.dataset.spf_dataset import v5_collate_beamsegnet, v5_thetas_to_targets\n",
    "from spf.model_training_and_inference.models.beamsegnet import (\n",
    "    BeamNSegNet,\n",
    "    BeamNetDirect,\n",
    "    BeamNetDiscrete,\n",
    "    ConvNet,\n",
    "    UNet1D,\n",
    ")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0,\n",
    "    \"collate_fn\": v5_collate_beamsegnet,\n",
    "}\n",
    "torch.manual_seed(1337)\n",
    "train_dataloader = torch.utils.data.DataLoader(ds, **dataloader_params)\n",
    "w = False\n",
    "if w:\n",
    "    import wandb\n",
    "\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"projectspf\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"beamsegnet1\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_instance(_x, _output_seg, _seg_mask, idx=0):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "    s = 0.3\n",
    "    axs[0].set_title(\"input (track 0/1)\")\n",
    "    axs[0].scatter(range(first_n), _x[idx, 0, :first_n], s=s)\n",
    "    axs[0].scatter(range(first_n), _x[idx, 1, :first_n], s=s)\n",
    "    axs[1].set_title(\"input (track 2)\")\n",
    "    axs[1].scatter(range(first_n), _x[idx, 2, :first_n], s=s)\n",
    "    # mw = mask_weights.cpu().detach().numpy()\n",
    "\n",
    "    axs[2].set_title(\"output vs gt\")\n",
    "    axs[2].scatter(range(first_n), _output_seg[idx, 0, :first_n], s=s)\n",
    "    axs[2].scatter(range(first_n), _seg_mask[idx, 0, :first_n], s=s)\n",
    "    return fig\n",
    "\n",
    "\n",
    "batch_data = next(iter(train_dataloader))\n",
    "import pickle\n",
    "\n",
    "pickle.dump(batch_data, open(\"test_batch.pkl\", \"wb\"))\n",
    "skip_segmentation = False\n",
    "segmentation_level = \"downsampled\"\n",
    "if segmentation_level == \"full\":\n",
    "    first_n = 10000\n",
    "    seg_m = UNet1D().to(torch_device)\n",
    "elif segmentation_level == \"downsampled\":\n",
    "    first_n = 256\n",
    "    seg_m = ConvNet(3, 1, 32, bn=True).to(torch_device)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "beam_m = BeamNetDirect(\n",
    "    nthetas=nthetas, hidden=16, symmetry=True, other=True, act=nn.SELU, bn=True\n",
    ").to(torch_device)\n",
    "# beam_m = BeamNetDiscrete(nthetas=nthetas, hidden=16, symmetry=False).to(torch_device)\n",
    "m = BeamNSegNet(segnet=seg_m, beamnet=beam_m, circular_mean=True).to(torch_device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(seg_m.parameters(), lr=0.01, weight_decay=0)\n",
    "\n",
    "step = 0\n",
    "head_start = 200\n",
    "for epoch in range(10000):\n",
    "    if step == head_start:\n",
    "        optimizer = torch.optim.AdamW(beam_m.parameters(), lr=0.001, weight_decay=0)\n",
    "        optimizer.zero_grad()\n",
    "    # for X, Y_rad in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # copy to torch device\n",
    "    if segmentation_level == \"full\":\n",
    "        x = batch_data[\"x\"].to(torch_device)\n",
    "        y_rad = batch_data[\"y_rad\"].to(torch_device)\n",
    "        seg_mask = batch_data[\"segmentation_mask\"].to(torch_device)\n",
    "    elif segmentation_level == \"downsampled\":\n",
    "        x = batch_data[\"all_windows_stats\"].to(torch_device)\n",
    "        y_rad = batch_data[\"y_rad\"].to(torch_device)\n",
    "        seg_mask = batch_data[\"downsampled_segmentation_mask\"].to(torch_device)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    assert seg_mask.ndim == 3 and seg_mask.shape[1] == 1\n",
    "\n",
    "    # run beamformer and segmentation\n",
    "    if not skip_segmentation:\n",
    "        output = m(x)\n",
    "    else:\n",
    "        output = m(x, seg_mask)\n",
    "\n",
    "    # x to beamformer loss (indirectly including segmentation)\n",
    "    x_to_beamformer_loss = -beam_m.loglikelihood(output[\"pred_theta\"], y_rad)\n",
    "    assert x_to_beamformer_loss.shape == (batch_size, 1)\n",
    "    x_to_beamformer_loss = x_to_beamformer_loss.mean()\n",
    "\n",
    "    # segmentation loss\n",
    "    x_to_segmentation_loss = (output[\"segmentation\"] - seg_mask) ** 2\n",
    "    assert x_to_segmentation_loss.ndim == 3 and x_to_segmentation_loss.shape[1] == 1\n",
    "    x_to_segmentation_loss = x_to_segmentation_loss.mean()\n",
    "\n",
    "    if skip_segmentation:\n",
    "        loss = x_to_beamformer_loss\n",
    "    else:\n",
    "        if step >= head_start:\n",
    "            loss = x_to_beamformer_loss\n",
    "        else:\n",
    "            loss = x_to_segmentation_loss\n",
    "    # if step in [799, 780]:\n",
    "    #     print(step, output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    to_log = {\n",
    "        \"loss\": loss.item(),\n",
    "        \"segmentation_loss\": x_to_segmentation_loss.item(),\n",
    "        \"beam_former_loss\": x_to_beamformer_loss.item(),\n",
    "    }\n",
    "    if step % 500 == 0:\n",
    "        # beam outputs\n",
    "        img_beam_output = (\n",
    "            (beam_m.render_discrete_x(output[\"pred_theta\"]) * 255).cpu().byte()\n",
    "        )\n",
    "        img_beam_gt = (beam_m.render_discrete_y(y_rad) * 255).cpu().byte()\n",
    "        train_target_image = torch.zeros(\n",
    "            (img_beam_output.shape[0] * 2, img_beam_output.shape[1]),\n",
    "        ).byte()\n",
    "        for row_idx in range(img_beam_output.shape[0]):\n",
    "            train_target_image[row_idx * 2] = img_beam_output[row_idx]\n",
    "            train_target_image[row_idx * 2 + 1] = img_beam_gt[row_idx]\n",
    "        if w:\n",
    "            output_image = wandb.Image(\n",
    "                train_target_image, caption=\"train vs target (interleaved)\"\n",
    "            )\n",
    "            to_log[\"output\"] = output_image\n",
    "\n",
    "        # segmentation output\n",
    "        _x = x.detach().cpu().numpy()\n",
    "        _seg_mask = seg_mask.detach().cpu().numpy()\n",
    "        # _output_seg = output_segmentation_upscaled.detach().cpu().numpy()\n",
    "        _output_seg = output[\"segmentation\"].detach().cpu().numpy()\n",
    "\n",
    "        fig = plot_instance(_x, _output_seg, _seg_mask, idx=0)\n",
    "        if w:\n",
    "            to_log[\"fig\"] = fig\n",
    "    if w:\n",
    "        wandb.log(to_log)\n",
    "    else:\n",
    "        # if step > 760 and step < 800:\n",
    "        if step % 20 == 0:\n",
    "            print(\n",
    "                step,\n",
    "                loss.item(),\n",
    "                x_to_beamformer_loss.item(),\n",
    "                x_to_segmentation_loss.item(),\n",
    "            )\n",
    "    step += 1\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "if w:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "z = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z(torch.Tensor([-10, 0, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_input = torch.mul(x, output[\"segmentation\"]).sum(axis=2) / output[\n",
    "    \"segmentation\"\n",
    "].sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = m.beamnet.fixify(m.beamnet.beam_net(weighted_input), sign=1)\n",
    "\n",
    "m.beamnet.likelihood(param, y_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param, y_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_param = param.clone()\n",
    "_param[:, 1] = 100\n",
    "m.beamnet.likelihood(_param, y_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rad.clamp(min=0, max=0.1)\n",
    "y_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.beamnet.beam_net(weighted_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.beamnet.beam_net(weighted_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"pred_theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_input = torch.mul(x, output[\"segmentation\"]).sum(axis=2) / output[\n",
    "    \"segmentation\"\n",
    "].sum(axis=2)\n",
    "weighted_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output[\"pred_theta\"][:, 0] - y_rad).shape, x_to_beamformer_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_m.loglikelihood(output[\"pred_theta\"], y_rad).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(output[\"beam_former\"], output[\"segmentation\"]).sum(axis=2) / output[\n",
    "    \"segmentation\"\n",
    "].sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = output[\"pred_theta\"]\n",
    "y = y_rad\n",
    "(x[:, 3] * torch.exp(-((x[:, 0] - y) ** 2) / x[:, 1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"pred_theta\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"beam_former\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    output_segmentation_upscaled = output[\"segmentation\"] * seg_mask.sum(\n",
    "        axis=2, keepdim=True\n",
    "    )\n",
    "    x_to_segmentation_loss = (output_segmentation_upscaled - seg_mask) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"mask_weights\"].shape, output[\"segmentation\"].shape, output[\"beam_former\"].shape\n",
    "k = torch.mul(output[\"beam_former\"], output[\"segmentation\"]) / output[\n",
    "    \"segmentation\"\n",
    "].sum(axis=2, keepdim=True)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_segmentation_upscaled = output[\"segmentation\"] * seg_mask.sum()\n",
    "# x_to_segmentation_loss = (output_segmentation_upscaled - seg_mask) ** 2\n",
    "(output[\"segmentation\"] * seg_mask.sum(axis=2, keepdim=True)).sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_mask.sum(axis=2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = output[\"segmentation\"].detach().cpu().numpy()[0, 0]\n",
    "# =_p_seg_mask[0,0]\n",
    "# z=_output_seg[0,0]\n",
    "plt.scatter(range(len(z)), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"segmentation\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 1, :].mean(), X[:, 1, :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape, Y_rad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation[0][\"all_windows_stats\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask(X, segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = X.clone().to(torch_device)\n",
    "_X[:, :2] /= 500\n",
    "batch_size, input_channels, session_size = _X.shape\n",
    "beam_former_input = _X.transpose(1, 2).reshape(\n",
    "    batch_size * session_size, input_channels\n",
    ")\n",
    "print(_X.device, beam_former_input)\n",
    "beam_former = m.beam_net(beam_former_input).reshape(\n",
    "    batch_size, session_size, 5  # mu, o1, o2, k1, k2\n",
    ")\n",
    "mask_weights = m.softmax(m.unet1d(_X)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_former_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_mask.cpu().detach().numpy()[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_n = 40000\n",
    "x = X[0].cpu()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axs[0].scatter(range(first_n), x[0, :first_n], s=0.3)\n",
    "axs[0].scatter(range(first_n), x[1, :first_n], s=0.3)\n",
    "axs[1].scatter(range(first_n), x[2, :first_n], s=0.3)\n",
    "# mw = mask_weights.cpu().detach().numpy()\n",
    "mw = m(X).cpu().detach().numpy()[0]\n",
    "axs[2].scatter(range(first_n), mw[0, :first_n], s=0.3)\n",
    "axs[2].scatter(range(first_n), seg_mask.cpu().detach().numpy()[0, :first_n], s=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.model_training_and_inference.models.beamsegnet import BeamNSegNetDirect\n",
    "\n",
    "\n",
    "m = BeamNSegNetDirect(nthetas=nthetas)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.01)\n",
    "\n",
    "m.beam_net.beam_net[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = x[[0]]\n",
    "k_y = y[[0]]\n",
    "k[:, 2] = -k[:, 2].sign() * k[:, 2]\n",
    "# k[:, 2] = k[:, 2].sign() * k[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "m.train()\n",
    "m.beam_net.beam_net[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = m(k)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "l = loss_fn(output, k_y)\n",
    "l.backward()\n",
    "# mean_loss = output\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.beam_net.beam_net[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Y.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def detrend_iq(iq_tensor):\n",
    "    print(iq_tensor.shape)\n",
    "    \"\"\"\n",
    "    Remove linear trends from the I and Q components of a 1D complex PyTorch tensor.\n",
    "    \n",
    "    Parameters:\n",
    "    - iq_tensor (torch.Tensor): 1D complex tensor of shape (N,), dtype=torch.complex64 or torch.complex128.\n",
    "    \n",
    "    Returns:\n",
    "    - detrended_iq (torch.Tensor): 1D complex tensor of shape (N,), with linear trends removed from I and Q.\n",
    "    \"\"\"\n",
    "    if not torch.is_complex(iq_tensor):\n",
    "        raise ValueError(\"Input tensor must be a complex tensor.\")\n",
    "    if iq_tensor.dim() != 1:\n",
    "        raise ValueError(\"Input tensor must be 1D.\")\n",
    "\n",
    "    # Number of samples\n",
    "    N = iq_tensor.shape[0]\n",
    "\n",
    "    # Time vector\n",
    "    t = torch.linspace(0, 1, steps=N, device=iq_tensor.device).unsqueeze(\n",
    "        1\n",
    "    )  # Shape: (N, 1)\n",
    "\n",
    "    # Design the design matrix for linear regression [t, 1]\n",
    "    X = torch.cat([t, torch.ones_like(t)], dim=1)  # Shape: (N, 2)\n",
    "\n",
    "    # Separate I and Q components\n",
    "    I = iq_tensor.real  # Shape: (N,)\n",
    "    Q = iq_tensor.imag  # Shape: (N,)\n",
    "\n",
    "    # Add a dimension for matrix operations\n",
    "    I = I.unsqueeze(1)  # Shape: (N, 1)\n",
    "    Q = Q.unsqueeze(1)  # Shape: (N, 1)\n",
    "\n",
    "    # Perform least squares linear regression to find slope and intercept\n",
    "    # Using torch.linalg.lstsq (available in PyTorch >=1.9)\n",
    "    coeffs_I = torch.linalg.lstsq(X, I).solution  # Shape: (2, 1)\n",
    "    coeffs_Q = torch.linalg.lstsq(X, Q).solution  # Shape: (2, 1)\n",
    "\n",
    "    # Compute the fitted trends\n",
    "    trend_I = X @ coeffs_I  # Shape: (N, 1)\n",
    "    trend_Q = X @ coeffs_Q  # Shape: (N, 1)\n",
    "\n",
    "    # Subtract the trends to detrend\n",
    "    I_detrended = I - trend_I  # Shape: (N, 1)\n",
    "    Q_detrended = Q - trend_Q  # Shape: (N, 1)\n",
    "\n",
    "    # Remove the extra dimension and combine into complex tensor\n",
    "    I_detrended = I_detrended.squeeze(1)  # Shape: (N,)\n",
    "    Q_detrended = Q_detrended.squeeze(1)  # Shape: (N,)\n",
    "    detrended_iq = torch.complex(I_detrended, Q_detrended)  # Shape: (N,)\n",
    "\n",
    "    return detrended_iq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
