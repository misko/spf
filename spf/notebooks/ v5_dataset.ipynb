{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = \"/Users/miskodzamba/Dropbox/research/gits/spf/\"\n",
    "repo_root = \"/home/mouse9911/gits/spf\"\n",
    "import sys\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)  # go to parent dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import yaml\n",
    "import torch\n",
    "from spf.rf import precompute_steering_vectors\n",
    "from spf.utils import zarr_open_from_lmdb_store\n",
    "from spf.dataset.v5_data import v5rx_f64_keys, v5rx_2xf64_keys\n",
    "import numpy as np\n",
    "from spf.rf import speed_of_light\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import os\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "#         return pickle.load(open(results_fn, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "\n",
    "ds = v5spfdataset(\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_04_05_22_13_07_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_10_05_03_21_nRX2_rx_circle_tag_rand10_90_rand30_100\"\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_04_10_05_08_55_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_06_19_04_15_nRX2_bounce\",\n",
    "    # \"test_circle2.zarr\",\n",
    "    # f\"{repo_root}/test_circle.zarr\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_18_16_29_28_nRX2_rx_circle.zarr\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_29_20_46_46_nRX2_rx_circle.zarr\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_20_11_59_44_nRX2_bounce.zarr\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_01_21_14_46_nRX2_rx_circle\",\n",
    "    # \"/Users/miskodzamba/Dropbox/research/gits/spf/wallarrayv3_2024_06_02_01_53_23_nRX2_bounce\",\n",
    "    # \"/Users/miskodzamba/Dropbox/research/gits/spf/wallarrayv3_2024_06_02_02_24_58_nRX2_bounce\",\n",
    "    # \"/Users/miskodzamba/Dropbox/research/gits/spf/wallarrayv3_2024_06_02_02_28_23_nRX2_bounce\",\n",
    "    # \"/Users/miskodzamba/Dropbox/research/gits/spf/wallarrayv3_2024_06_02_02_34_03_nRX2_bounce\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_02_05_18_27_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_02_06_47_47_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/allarrayv3_2024_06_02_08_17_31_nRX2_bounce\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_04_19_14_43_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_04_19_44_10_nRX2_bounce\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_04_21_10_37_nRX2_bounce\",\n",
    "    # \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_06_05_02_41_50_nRX2_rx_circle\",\n",
    "    # \"/Volumes/SPFData2/missions/april5/wallarrayv3_2024_06_07_23_43_19_nRX2_bounce\",\n",
    "    # \"/mnt/md0/spf/2d_wallarray_v2_data/june/wallarrayv3_2024_06_10_14_00_06_nRX2_rx_circle\",\n",
    "    # \"/mnt/md0/spf/2d_wallarray_v2_data/june/wallarrayv3_2024_06_03_01_40_27_nRX2_bounce.zarr\",\n",
    "    # \"/mnt/md0/spf/2d_wallarray_v2_data/june/wallarrayv3_2024_06_03_00_30_00_nRX2_rx_circle.zarr\",\n",
    "    \"/mnt/md0/spf/2d_wallarray_v2_data/june/wallarrayv3_2024_06_03_01_17_43_nRX2_rx_circle.zarr\",\n",
    "    nthetas=129,\n",
    "    ignore_qc=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.get_ground_truth_thetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.average_windows_in_segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(\n",
    "    [\n",
    "        [\n",
    "            len(x[\"simple_segmentation\"]) > 2\n",
    "            for x in ds.segmentation[\"segmentation_by_receiver\"][f\"r{rx_idx}\"]\n",
    "        ]\n",
    "        for rx_idx in [0, 1]\n",
    "    ]\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_idx = 150 * 2 + 100\n",
    "session_idx = 1887\n",
    "session_idx = len(ds) - 100\n",
    "session_idx = 5\n",
    "session_idx = 5\n",
    "session_idx = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_sds[0].shape, v.shape\n",
    "\n",
    "# window_size = 2500\n",
    "# v_windowed = v.reshape(2, -1, window_size).sum(axis=2)\n",
    "# receiver_idx = 0\n",
    "\n",
    "# window_sds = beamformer_given_steering_nomean(\n",
    "#     steering_vectors=ds.steering_vectors[receiver_idx],\n",
    "#     signal_matrix=v,\n",
    "# )\n",
    "# v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(window_sds.reshape(65, -1, window_size).mean(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(windows_sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import beamformer_given_steering_nomean, get_phase_diff, simple_segment\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = ds[session_idx][0]\n",
    "# signal_matrix = load_zarr_to_numpy(z.receivers[\"r0\"].signal_matrix[session_idx])\n",
    "n = 2 * 4 * 50000\n",
    "offset = 0\n",
    "v = data[\"signal_matrix\"][:, offset : offset + n].numpy()\n",
    "pd = get_phase_diff(v)\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 6))\n",
    "\n",
    "axs[0].scatter(np.arange(n), np.abs(v[0]), alpha=0.1, s=1, label=\"ant0\")\n",
    "axs[0].scatter(np.arange(n), np.abs(v[1]), alpha=0.1, s=1, label=\"ant1\")\n",
    "axs[0].set_title(\"Raw signal\")\n",
    "axs[0].legend()\n",
    "axs[0].set_xlabel(\"Sample# (time)\")\n",
    "axs[1].set_xlabel(\"Sample# (time)\")\n",
    "axs[1].set_title(\"Phase estimates\")\n",
    "axs[1].scatter(np.arange(n), pd, s=1, alpha=0.1)\n",
    "\n",
    "beam_sds = [\n",
    "    beamformer_given_steering_nomean(\n",
    "        steering_vectors=ds.steering_vectors[receiver_idx],\n",
    "        signal_matrix=v,\n",
    "    )\n",
    "    for receiver_idx in range(2)\n",
    "]\n",
    "\n",
    "window_sds = []\n",
    "for window in simple_segment(\n",
    "    v,\n",
    "    window_size=2500,\n",
    "    stride=2500,\n",
    "    trim=20,\n",
    "    mean_diff_threshold=0.2,  #\n",
    "    max_stddev_threshold=0.5,  # just eyeballed this\n",
    "    drop_less_than_size=3000,\n",
    "    min_abs_signal=40,\n",
    ")[\"simple_segmentation\"]:\n",
    "    if window[\"type\"] == \"signal\":\n",
    "        axs[1].plot(\n",
    "            [window[\"start_idx\"], window[\"end_idx\"]],\n",
    "            [window[\"mean\"], window[\"mean\"]],\n",
    "            color=\"red\",\n",
    "        )\n",
    "    else:\n",
    "        axs[1].plot(\n",
    "            [window[\"start_idx\"], window[\"end_idx\"]],\n",
    "            [window[\"mean\"], window[\"mean\"]],\n",
    "            color=\"orange\",\n",
    "        )\n",
    "    # print(window[\"start_idx\"], window[\"end_idx\"])\n",
    "    _beam_sds = beam_sds[0][:, window[\"start_idx\"] : window[\"end_idx\"]].mean(axis=1)\n",
    "    # _beam_sds = _beam_sds.mean(axis=1)\n",
    "    # _beam_sds -= _beam_sds.min()\n",
    "    window_sds.append(_beam_sds)\n",
    "window_sds = np.array(window_sds)\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.segmentation[\"segmentation_by_receiver\"][\"r0\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[session_idx][0][\"windowed_beamformer\"][None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[session_idx + 1650][0][\"windowed_beamformer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[session_idx + 1650][0][\"windowed_beamformer\"].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(\n",
    "    ds.segmentation[\"segmentation_by_receiver\"][\"r0\"][session_idx // 2][\n",
    "        \"windowed_beamformer\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ground_truth_thetas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import thetas_from_nthetas\n",
    "\n",
    "\n",
    "thetas_from_nthetas(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.rx_configs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.receiver_data[0][\"rx_pos_x_mm\"][0], ds.receiver_data[0][\"rx_pos_y_mm\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ground_truth_thetas[1][0] / np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(window_sds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[session_idx][0][\"simple_segmentation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(\n",
    "    [\n",
    "        len(x[\"simple_segmentation\"])\n",
    "        for x in ds.segmentation[\"segmentation_by_receiver\"][\"r1\"]\n",
    "    ]\n",
    ").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.ground_truth_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.rf import pi_norm\n",
    "\n",
    "\n",
    "ridx = 0\n",
    "rx_theta_in_pis = ds.receiver_data[ridx][\"rx_theta_in_pis\"]\n",
    "tx_pos = np.array(\n",
    "    [\n",
    "        ds.receiver_data[ridx][\"tx_pos_x_mm\"],\n",
    "        ds.receiver_data[ridx][\"tx_pos_y_mm\"],\n",
    "    ]\n",
    ")\n",
    "rx_pos = np.array(\n",
    "    [\n",
    "        ds.receiver_data[ridx][\"rx_pos_x_mm\"],\n",
    "        ds.receiver_data[ridx][\"rx_pos_y_mm\"],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compute the angle of the tx with respect to rx\n",
    "d = tx_pos - rx_pos\n",
    "\n",
    "rx_to_tx_theta = np.arctan2(d[0], d[1])\n",
    "theta = pi_norm(rx_to_tx_theta - rx_theta_in_pis[:] * np.pi)\n",
    "theta, ds.get_ground_truth_thetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.yaml_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = ds.get_segmentation()\n",
    "mean_phase_results = {}\n",
    "for receiver, results in segmentation[\"segmentation_by_receiver\"].items():\n",
    "    mean_phase_results[receiver] = np.array(\n",
    "        [\n",
    "            np.array([x[\"mean\"] for x in result[\"simple_segmentation\"]]).mean()\n",
    "            for result in results\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "first_n = 512  # 12 * 8\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "axs.scatter(range(first_n), mean_phase_results[\"r0\"][:first_n], s=3, label=\"Rx0\")\n",
    "axs.scatter(range(first_n), mean_phase_results[\"r1\"][:first_n], s=3, label=\"Rx1\")\n",
    "axs.legend()\n",
    "axs.axvline(x=115)\n",
    "axs.set_title(\"Mean segmented phase diff\")\n",
    "axs.set_xlabel(\"Chunk (time)\")\n",
    "axs.set_ylabel(\"Mean phase diff of seg. chunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_phase_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_by_receiver.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.get_segmentation_mean_phase()\n",
    "ds.get_estimated_thetas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import pi_norm\n",
    "from spf.rf import c as speed_of_light\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "estimated_thetas = ds.get_estimated_thetas()\n",
    "for rx_idx in [0, 1]:\n",
    "\n",
    "    axs[rx_idx].scatter(\n",
    "        range(estimated_thetas[f\"r{rx_idx}\"][0].shape[0]),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][0]),\n",
    "        s=0.4,\n",
    "    )\n",
    "    axs[rx_idx].scatter(\n",
    "        range(estimated_thetas[f\"r{rx_idx}\"][1].shape[0]),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][1]),\n",
    "        s=0.4,\n",
    "    )\n",
    "    axs[rx_idx].scatter(\n",
    "        range(estimated_thetas[f\"r{rx_idx}\"][2].shape[0]),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][2]),\n",
    "        s=0.4,\n",
    "    )\n",
    "    axs[rx_idx].set_xlabel(\"Chunk\")\n",
    "    axs[rx_idx].set_ylabel(\"estimated theta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.dataset.spf_dataset import pi_norm\n",
    "from spf.rf import reduce_theta_to_positive_y\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "first_n = 3000\n",
    "estimated_thetas = ds.get_estimated_thetas()\n",
    "for rx_idx in [0, 1]:\n",
    "    expected_theta = ds.ground_truth_thetas[rx_idx]\n",
    "    axs[rx_idx].plot(\n",
    "        expected_theta[:first_n], alpha=1, color=\"red\", label=\"ground truth\"\n",
    "    )\n",
    "    axs[rx_idx].plot(\n",
    "        reduce_theta_to_positive_y(expected_theta[:first_n]),\n",
    "        alpha=1,\n",
    "        color=\"green\",\n",
    "        label=\"reduced ground truth\",\n",
    "    )\n",
    "\n",
    "    n = estimated_thetas[f\"r{rx_idx}\"][0].shape[0]\n",
    "    axs[rx_idx].scatter(\n",
    "        range(first_n),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][0])[:first_n],\n",
    "        s=3,\n",
    "        label=f\"Rx{rx_idx}_peak1\",\n",
    "    )\n",
    "    axs[rx_idx].scatter(\n",
    "        range(first_n),\n",
    "        pi_norm(estimated_thetas[f\"r{rx_idx}\"][1])[:first_n],\n",
    "        s=3,\n",
    "        label=f\"Rx{rx_idx}_peak2\",\n",
    "    )\n",
    "    axs[rx_idx].set_xlabel(\"Chunk\")\n",
    "    axs[rx_idx].set_ylabel(\"estimated theta\")\n",
    "    axs[rx_idx].legend()\n",
    "    axs[rx_idx].set_title(f\"Receiver (Rx) {rx_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = \"/Users/miskodzamba/Dropbox/research/gits/spf/\"\n",
    "import sys\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)  # go to parent dir\n",
    "\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "\n",
    "\n",
    "ds = v5spfdataset(\n",
    "    \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_06_19_04_15_nRX2_bounce\",\n",
    "    nthetas=11,\n",
    ")\n",
    "\n",
    "from functools import cache\n",
    "import gc\n",
    "\n",
    "from spf.dataset.spf_dataset import v5_collate_beamsegnet, v5_thetas_to_targets\n",
    "from spf.model_training_and_inference.models.beamsegnet import (\n",
    "    BeamNSegNetDirect,\n",
    "    BeamNSegNetDiscrete,\n",
    "    # BeamNetDirect,\n",
    "    UNet1D,\n",
    "    ConvNet,\n",
    ")\n",
    "\n",
    "torch_device = torch.device(\"cpu\")\n",
    "nthetas = 11\n",
    "lr = 0.001\n",
    "\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": 4,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0,\n",
    "    \"collate_fn\": v5_collate_beamsegnet,\n",
    "}\n",
    "torch.manual_seed(1337)\n",
    "train_dataloader = torch.utils.data.DataLoader(ds, **dataloader_params)\n",
    "\n",
    "import random\n",
    "\n",
    "w = False\n",
    "if w:\n",
    "\n",
    "    import wandb\n",
    "\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"projectspf\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"beamsegnet1\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@cache\n",
    "def mean_guess(shape):\n",
    "    return torch.nn.functional.normalize(torch.ones(shape), p=1, dim=1)\n",
    "\n",
    "\n",
    "X, Y_rad, segmentation = next(iter(train_dataloader))\n",
    "\n",
    "\n",
    "def batch_to_gt_segmentation(X, Y_rad, segmentation):\n",
    "    n, _, samples_per_session = X.shape\n",
    "    window_size = 2048\n",
    "    stride = 2048\n",
    "    assert window_size == stride\n",
    "    assert samples_per_session % window_size == 0\n",
    "    n_windows = samples_per_session // window_size\n",
    "    window_status = torch.zeros(n, n_windows)\n",
    "    for row_idx in range(len(segmentation)):\n",
    "        for window in segmentation[row_idx][\"simple_segmentation\"]:\n",
    "            window_status[\n",
    "                row_idx,\n",
    "                window[\"start_idx\"] // window_size : window[\"end_idx\"] // window_size,\n",
    "            ] = 1\n",
    "    return window_status[:, None]\n",
    "\n",
    "\n",
    "def segmentation_mask(X, segmentations):\n",
    "    seg_mask = torch.zeros(\n",
    "        X.shape[0], X.shape[2], device=X.device\n",
    "    )  # X.new(X.shape[0], X.shape[2])\n",
    "    for row_idx in range(seg_mask.shape[0]):\n",
    "        for w in segmentations[row_idx][\"simple_segmentation\"]:\n",
    "            seg_mask[row_idx, w[\"start_idx\"] : w[\"end_idx\"]] = 1\n",
    "    return seg_mask[:, None]  # orch.nn.functional.normalize(seg_mask, p=1, dim=1)\n",
    "\n",
    "\n",
    "# m = BeamNSegNetDiscrete(nthetas=nthetas, symmetry=False).to(torch_device)\n",
    "# m = BeamNSegNetDirect(nthetas=nthetas, symmetry=False).to(torch_device)\n",
    "# print(\"ALL\", segmentation[0][\"all_windows_stats\"].shape)\n",
    "m = UNet1D().to(torch_device).double()\n",
    "# m = ConvNet(in_channels=3, out_channels=1, hidden=32)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.00001, weight_decay=0)\n",
    "step = 0\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "X = X.double().to(torch_device)\n",
    "# X[:, :2] /= 500\n",
    "for epoch in range(10000):\n",
    "    # for X, Y_rad, segmentation in train_dataloader:\n",
    "    if True:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # full\n",
    "        input = X.clone().to(torch_device)\n",
    "        output = m(input)\n",
    "\n",
    "        seg_mask = segmentation_mask(X, segmentation)\n",
    "        print(input.shape, output.shape, seg_mask.shape)\n",
    "\n",
    "        # downsampled\n",
    "        # input = torch.Tensor(\n",
    "        #     np.vstack(\n",
    "        #         [\n",
    "        #             segmentation[idx][\"all_windows_stats\"].transpose()[None]\n",
    "        #             for idx in range(len(segmentation))\n",
    "        #         ]\n",
    "        #     )\n",
    "        # )\n",
    "        # input[:, 2] /= 50\n",
    "        # output = m(input)\n",
    "        # seg_mask = batch_to_gt_segmentation(X, Y_rad, segmentation)\n",
    "\n",
    "        loss = ((output - seg_mask) ** 2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        to_log = {\"loss\": loss.item()}\n",
    "\n",
    "        _input = input.cpu()\n",
    "        _output = output.cpu().detach().numpy()\n",
    "        first_n = 3000\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(loss.item())\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "            s = 0.3\n",
    "            axs[0].set_title(\"input (track 0/1)\")\n",
    "            axs[0].scatter(range(first_n), _input[0, 0, :first_n], s=s)\n",
    "            axs[0].scatter(range(first_n), _input[0, 1, :first_n], s=s)\n",
    "            axs[1].set_title(\"input (track 2)\")\n",
    "            axs[1].scatter(range(first_n), _input[0, 2, :first_n], s=s)\n",
    "            # mw = mask_weights.cpu().detach().numpy()\n",
    "\n",
    "            axs[2].set_title(\"output vs gt\")\n",
    "            axs[2].scatter(range(first_n), _output[0, 0, :first_n], s=s)\n",
    "            axs[2].scatter(\n",
    "                range(first_n), seg_mask.cpu().detach().numpy()[0, 0, :first_n], s=s\n",
    "            )\n",
    "            to_log[\"fig\"] = fig\n",
    "        if w:\n",
    "            wandb.log(to_log)\n",
    "        step += 1\n",
    "\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape, seg_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = \"/Users/miskodzamba/Dropbox/research/gits/spf/\"\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)  # go to parent dir\n",
    "\n",
    "from spf.dataset.spf_dataset import v5spfdataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch_device = torch.device(\"cpu\")\n",
    "nthetas = 11\n",
    "lr = 0.001\n",
    "batch_size = 8\n",
    "\n",
    "ds = v5spfdataset(\n",
    "    \"/Volumes/SPFData/missions/april5/wallarrayv3_2024_05_06_19_04_15_nRX2_bounce\",\n",
    "    nthetas=11,\n",
    ")\n",
    "\n",
    "from functools import cache\n",
    "import gc\n",
    "\n",
    "from spf.dataset.spf_dataset import v5_collate_beamsegnet, v5_thetas_to_targets\n",
    "from spf.model_training_and_inference.models.beamsegnet import (\n",
    "    BeamNSegNet,\n",
    "    BeamNetDirect,\n",
    "    BeamNetDiscrete,\n",
    "    ConvNet,\n",
    "    UNet1D,\n",
    ")\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0,\n",
    "    \"collate_fn\": v5_collate_beamsegnet,\n",
    "}\n",
    "torch.manual_seed(1337)\n",
    "train_dataloader = torch.utils.data.DataLoader(ds, **dataloader_params)\n",
    "w = False\n",
    "if w:\n",
    "    import wandb\n",
    "\n",
    "    # start a new wandb run to track this script\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"projectspf\",\n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": lr,\n",
    "            \"architecture\": \"beamsegnet1\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_instance(_x, _output_seg, _seg_mask, idx=0):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "    s = 0.3\n",
    "    axs[0].set_title(\"input (track 0/1)\")\n",
    "    axs[0].scatter(range(first_n), _x[idx, 0, :first_n], s=s)\n",
    "    axs[0].scatter(range(first_n), _x[idx, 1, :first_n], s=s)\n",
    "    axs[1].set_title(\"input (track 2)\")\n",
    "    axs[1].scatter(range(first_n), _x[idx, 2, :first_n], s=s)\n",
    "    # mw = mask_weights.cpu().detach().numpy()\n",
    "\n",
    "    axs[2].set_title(\"output vs gt\")\n",
    "    axs[2].scatter(range(first_n), _output_seg[idx, 0, :first_n], s=s)\n",
    "    axs[2].scatter(range(first_n), _seg_mask[idx, 0, :first_n], s=s)\n",
    "    return fig\n",
    "\n",
    "\n",
    "batch_data = next(iter(train_dataloader))\n",
    "import pickle\n",
    "\n",
    "pickle.dump(batch_data, open(\"test_batch.pkl\", \"wb\"))\n",
    "skip_segmentation = False\n",
    "segmentation_level = \"downsampled\"\n",
    "if segmentation_level == \"full\":\n",
    "    first_n = 10000\n",
    "    seg_m = UNet1D().to(torch_device)\n",
    "elif segmentation_level == \"downsampled\":\n",
    "    first_n = 256\n",
    "    seg_m = ConvNet(3, 1, 32, bn=True).to(torch_device)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "beam_m = BeamNetDirect(\n",
    "    nthetas=nthetas, hidden=16, symmetry=True, other=True, act=nn.SELU, bn=True\n",
    ").to(torch_device)\n",
    "# beam_m = BeamNetDiscrete(nthetas=nthetas, hidden=16, symmetry=False).to(torch_device)\n",
    "m = BeamNSegNet(segnet=seg_m, beamnet=beam_m, circular_mean=True).to(torch_device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(seg_m.parameters(), lr=0.01, weight_decay=0)\n",
    "\n",
    "step = 0\n",
    "head_start = 200\n",
    "for epoch in range(10000):\n",
    "    if step == head_start:\n",
    "        optimizer = torch.optim.AdamW(beam_m.parameters(), lr=0.001, weight_decay=0)\n",
    "        optimizer.zero_grad()\n",
    "    # for X, Y_rad in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # copy to torch device\n",
    "    if segmentation_level == \"full\":\n",
    "        x = batch_data[\"x\"].to(torch_device)\n",
    "        y_rad = batch_data[\"y_rad\"].to(torch_device)\n",
    "        seg_mask = batch_data[\"segmentation_mask\"].to(torch_device)\n",
    "    elif segmentation_level == \"downsampled\":\n",
    "        x = batch_data[\"all_windows_stats\"].to(torch_device)\n",
    "        y_rad = batch_data[\"y_rad\"].to(torch_device)\n",
    "        seg_mask = batch_data[\"downsampled_segmentation_mask\"].to(torch_device)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    assert seg_mask.ndim == 3 and seg_mask.shape[1] == 1\n",
    "\n",
    "    # run beamformer and segmentation\n",
    "    if not skip_segmentation:\n",
    "        output = m(x)\n",
    "    else:\n",
    "        output = m(x, seg_mask)\n",
    "\n",
    "    # x to beamformer loss (indirectly including segmentation)\n",
    "    x_to_beamformer_loss = -beam_m.loglikelihood(output[\"pred_theta\"], y_rad)\n",
    "    assert x_to_beamformer_loss.shape == (batch_size, 1)\n",
    "    x_to_beamformer_loss = x_to_beamformer_loss.mean()\n",
    "\n",
    "    # segmentation loss\n",
    "    x_to_segmentation_loss = (output[\"segmentation\"] - seg_mask) ** 2\n",
    "    assert x_to_segmentation_loss.ndim == 3 and x_to_segmentation_loss.shape[1] == 1\n",
    "    x_to_segmentation_loss = x_to_segmentation_loss.mean()\n",
    "\n",
    "    if skip_segmentation:\n",
    "        loss = x_to_beamformer_loss\n",
    "    else:\n",
    "        if step >= head_start:\n",
    "            loss = x_to_beamformer_loss\n",
    "        else:\n",
    "            loss = x_to_segmentation_loss\n",
    "    # if step in [799, 780]:\n",
    "    #     print(step, output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    to_log = {\n",
    "        \"loss\": loss.item(),\n",
    "        \"segmentation_loss\": x_to_segmentation_loss.item(),\n",
    "        \"beam_former_loss\": x_to_beamformer_loss.item(),\n",
    "    }\n",
    "    if step % 500 == 0:\n",
    "        # beam outputs\n",
    "        img_beam_output = (\n",
    "            (beam_m.render_discrete_x(output[\"pred_theta\"]) * 255).cpu().byte()\n",
    "        )\n",
    "        img_beam_gt = (beam_m.render_discrete_y(y_rad) * 255).cpu().byte()\n",
    "        train_target_image = torch.zeros(\n",
    "            (img_beam_output.shape[0] * 2, img_beam_output.shape[1]),\n",
    "        ).byte()\n",
    "        for row_idx in range(img_beam_output.shape[0]):\n",
    "            train_target_image[row_idx * 2] = img_beam_output[row_idx]\n",
    "            train_target_image[row_idx * 2 + 1] = img_beam_gt[row_idx]\n",
    "        if w:\n",
    "            output_image = wandb.Image(\n",
    "                train_target_image, caption=\"train vs target (interleaved)\"\n",
    "            )\n",
    "            to_log[\"output\"] = output_image\n",
    "\n",
    "        # segmentation output\n",
    "        _x = x.detach().cpu().numpy()\n",
    "        _seg_mask = seg_mask.detach().cpu().numpy()\n",
    "        # _output_seg = output_segmentation_upscaled.detach().cpu().numpy()\n",
    "        _output_seg = output[\"segmentation\"].detach().cpu().numpy()\n",
    "\n",
    "        fig = plot_instance(_x, _output_seg, _seg_mask, idx=0)\n",
    "        if w:\n",
    "            to_log[\"fig\"] = fig\n",
    "    if w:\n",
    "        wandb.log(to_log)\n",
    "    else:\n",
    "        # if step > 760 and step < 800:\n",
    "        if step % 20 == 0:\n",
    "            print(\n",
    "                step,\n",
    "                loss.item(),\n",
    "                x_to_beamformer_loss.item(),\n",
    "                x_to_segmentation_loss.item(),\n",
    "            )\n",
    "    step += 1\n",
    "\n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "if w:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "z = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z(torch.Tensor([-10, 0, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_input = torch.mul(x, output[\"segmentation\"]).sum(axis=2) / output[\n",
    "    \"segmentation\"\n",
    "].sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = m.beamnet.fixify(m.beamnet.beam_net(weighted_input), sign=1)\n",
    "\n",
    "m.beamnet.likelihood(param, y_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param, y_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_param = param.clone()\n",
    "_param[:, 1] = 100\n",
    "m.beamnet.likelihood(_param, y_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_rad.clamp(min=0, max=0.1)\n",
    "y_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.beamnet.beam_net(weighted_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.beamnet.beam_net(weighted_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"pred_theta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_input = torch.mul(x, output[\"segmentation\"]).sum(axis=2) / output[\n",
    "    \"segmentation\"\n",
    "].sum(axis=2)\n",
    "weighted_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(output[\"pred_theta\"][:, 0] - y_rad).shape, x_to_beamformer_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_m.loglikelihood(output[\"pred_theta\"], y_rad).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mul(output[\"beam_former\"], output[\"segmentation\"]).sum(axis=2) / output[\n",
    "    \"segmentation\"\n",
    "].sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = output[\"pred_theta\"]\n",
    "y = y_rad\n",
    "(x[:, 3] * torch.exp(-((x[:, 0] - y) ** 2) / x[:, 1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"pred_theta\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"beam_former\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    output_segmentation_upscaled = output[\"segmentation\"] * seg_mask.sum(\n",
    "        axis=2, keepdim=True\n",
    "    )\n",
    "    x_to_segmentation_loss = (output_segmentation_upscaled - seg_mask) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"mask_weights\"].shape, output[\"segmentation\"].shape, output[\"beam_former\"].shape\n",
    "k = torch.mul(output[\"beam_former\"], output[\"segmentation\"]) / output[\n",
    "    \"segmentation\"\n",
    "].sum(axis=2, keepdim=True)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_segmentation_upscaled = output[\"segmentation\"] * seg_mask.sum()\n",
    "# x_to_segmentation_loss = (output_segmentation_upscaled - seg_mask) ** 2\n",
    "(output[\"segmentation\"] * seg_mask.sum(axis=2, keepdim=True)).sum(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_mask.sum(axis=2, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = output[\"segmentation\"].detach().cpu().numpy()[0, 0]\n",
    "# =_p_seg_mask[0,0]\n",
    "# z=_output_seg[0,0]\n",
    "plt.scatter(range(len(z)), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"segmentation\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 1, :].mean(), X[:, 1, :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape, Y_rad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation[0][\"all_windows_stats\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_mask(X, segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = X.clone().to(torch_device)\n",
    "_X[:, :2] /= 500\n",
    "batch_size, input_channels, session_size = _X.shape\n",
    "beam_former_input = _X.transpose(1, 2).reshape(\n",
    "    batch_size * session_size, input_channels\n",
    ")\n",
    "print(_X.device, beam_former_input)\n",
    "beam_former = m.beam_net(beam_former_input).reshape(\n",
    "    batch_size, session_size, 5  # mu, o1, o2, k1, k2\n",
    ")\n",
    "mask_weights = m.softmax(m.unet1d(_X)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_former_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_mask.cpu().detach().numpy()[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_n = 40000\n",
    "x = X[0].cpu()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axs[0].scatter(range(first_n), x[0, :first_n], s=0.3)\n",
    "axs[0].scatter(range(first_n), x[1, :first_n], s=0.3)\n",
    "axs[1].scatter(range(first_n), x[2, :first_n], s=0.3)\n",
    "# mw = mask_weights.cpu().detach().numpy()\n",
    "mw = m(X).cpu().detach().numpy()[0]\n",
    "axs[2].scatter(range(first_n), mw[0, :first_n], s=0.3)\n",
    "axs[2].scatter(range(first_n), seg_mask.cpu().detach().numpy()[0, :first_n], s=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spf.model_training_and_inference.models.beamsegnet import BeamNSegNetDirect\n",
    "\n",
    "\n",
    "m = BeamNSegNetDirect(nthetas=nthetas)\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=0.01)\n",
    "\n",
    "m.beam_net.beam_net[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = x[[0]]\n",
    "k_y = y[[0]]\n",
    "k[:, 2] = -k[:, 2].sign() * k[:, 2]\n",
    "# k[:, 2] = k[:, 2].sign() * k[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "m.train()\n",
    "m.beam_net.beam_net[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = m(k)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "l = loss_fn(output, k_y)\n",
    "l.backward()\n",
    "# mean_loss = output\n",
    "# optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.beam_net.beam_net[0].weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Y.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
